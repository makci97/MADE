{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"resources/made.jpg\" width=\"35%\" height=\"35%\">\n",
    "\n",
    "# Академия MADE\n",
    "## Семинар 5: регуляризация / аугментация / OpenCV\n",
    "Иван Карпухин, ведущий программист-исследователь команды машинного зрения\n",
    "\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "Семинар состоит из двух частей. В первой мы применим классические методы регуляризации. Во второй сделаем аугментацию данных средствами OpenCV.\n",
    "\n",
    "Для выполнения работы нужны следующие пакеты (Python 3):\n",
    "* opencv-python\n",
    "* torch\n",
    "* torchvision\n",
    "\n",
    "Установить их можно командой:\n",
    "```bash\n",
    "pip3 install --user opencv-python torch torchvision\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import cvmade\n",
    "import seminar\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "# Параметры тренировки.\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    print(\"Use CUDA\")\n",
    "\n",
    "\n",
    "TRAIN = False\n",
    "\n",
    "TRAIN_KWARGS = {\"plot\": True,\n",
    "                \"train_plot_kwargs\": {\"c\": \"b\"},\n",
    "                \"test_scatter_kwargs\": {\"c\": \"y\", \"s\": 100, \"zorder\": 1e10},\n",
    "                \"use_cuda\": USE_CUDA}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Простая тренировка на CIFAR-10.\n",
    "\n",
    "## Подготовка данных\n",
    "\n",
    "Если возникнут трудности при загрузке данных, можно скачать архив по ссылке и самостоятельно распаковать в папку с ноутбуком, так, чтобы рядом с ноутбуком появилась папка cifar-10-batches-py.\n",
    "\n",
    "Ссылка с данными (если не заработает автоскачивание): https://cloud.mail.ru/public/2ku1/KJPEsJ6xR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "DATA_ROOT = \".\"\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),  # Преобразовать PIL изображения в Torch тензоры.\n",
    "    torchvision.transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "trainset = CIFAR10(DATA_ROOT, train=True, download=True, transform=transform)\n",
    "testset = CIFAR10(DATA_ROOT, train=False, download=True, transform=transform)\n",
    "print(\"Single image shape (CHW):\", list(testset[0][0].shape))\n",
    "print(\"Trainset size:\", len(trainset))\n",
    "print(\"Testset size:\", len(testset))\n",
    "\n",
    "print(\"Trainset\")\n",
    "cvmade.plot.torch.show_images_dataset(trainset)\n",
    "print(\"Testset\")\n",
    "cvmade.plot.torch.show_images_dataset(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание модели\n",
    "\n",
    "Создадим сеть вида VGG (Сокращение от Visual Geometry Group, https://arxiv.org/pdf/1409.1556.pdf ).\n",
    "\n",
    "Сеть VGG состоит из сверток 3x3, ReLU, max pooling и полносвязных слоев. Оригинальная сеть VGG применялась к изображениям 224x224, мы же применяем к изображениям 32x32. В статье описаны варианты сети VGG с количеством слоев от 11 до 19. Для задачи CIFAR10 достаточно меньшего числа слоев. Итоговая сложность модели позволит нам учить сеть на CPU.\n",
    "\n",
    "Заметьте, что после pooling слоя площадь изображения уменьшается в 4 раза, а число каналов увеличивается в два раза, т.е. размер тензора уменьшается в два раза. Баланс между размером изображения и числом каналов позволяет сохранить достаточное количество информации при продвижении по сети.\n",
    "\n",
    "<img src=\"resources/vgg1.jpg\" align=\"left\" hspace=\"20\" width=\"50%\" height=\"50%\"/> \n",
    "<img src=\"resources/vgg2.jpg\" align=\"left\" hspace=\"20\" width=\"50%\" height=\"50%\"/>\n",
    "<div style=\"clear:both;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шпаргалка:\n",
    "\n",
    "```python\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "\n",
    "torch.nn.ReLU(inplace=False)\n",
    "\n",
    "torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "\n",
    "torch.nn.Linear(in_features, out_features, bias=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class VGGNetwork(torch.nn.Sequential):        \n",
    "    def _make_conv3x3(self, in_channels, out_channels):\n",
    "        # Начало вашего кода.\n",
    "        \n",
    "        layer = ...\n",
    "        \n",
    "        # Конец вашего кода.\n",
    "        return layer\n",
    "    \n",
    "    def _make_relu(self):\n",
    "        # Начало вашего кода.\n",
    "        \n",
    "        layer = ...\n",
    "        \n",
    "        # Конец вашего кода.\n",
    "        return layer\n",
    "    \n",
    "    def _make_maxpool2x2(self):\n",
    "        # Начало вашего кода.\n",
    "        \n",
    "        layer = ...\n",
    "        \n",
    "        # Конец вашего кода.\n",
    "        return layer\n",
    "    \n",
    "    def _make_fully_connected(self, in_features, out_features, last=False):\n",
    "        # Начало вашего кода.\n",
    "        \n",
    "        layer = ...\n",
    "        \n",
    "        # Конец вашего кода.\n",
    "        return layer\n",
    "    \n",
    "    def __init__(self, n_classes=10):\n",
    "        # Мы используем same padding, чтобы свертки не меняли ширину и высоту тензоров.\n",
    "        # Ширина и высота меняются только в maxpool (уменьшаются в два раза).\n",
    "        layers = [\n",
    "            self._make_conv3x3(in_channels=3, out_channels=32),\n",
    "            self._make_conv3x3(in_channels=32, out_channels=32),\n",
    "            self._make_relu(),\n",
    "            self._make_maxpool2x2(),\n",
    "            \n",
    "            self._make_conv3x3(in_channels=32, out_channels=64),\n",
    "            self._make_conv3x3(in_channels=64, out_channels=64),\n",
    "            self._make_relu(),\n",
    "            self._make_maxpool2x2(),\n",
    "            \n",
    "            self._make_conv3x3(in_channels=64, out_channels=128),\n",
    "            self._make_conv3x3(in_channels=128, out_channels=128),\n",
    "            self._make_relu(),\n",
    "            self._make_maxpool2x2(),\n",
    "            \n",
    "            self._make_conv3x3(in_channels=128, out_channels=256),\n",
    "            self._make_conv3x3(in_channels=256, out_channels=256),\n",
    "            self._make_relu(),\n",
    "            self._make_maxpool2x2(),\n",
    "            \n",
    "            torch.nn.modules.Flatten(),  # Преобразовать 4-мерный тензор BHWC в двумерный BD.\n",
    "            \n",
    "            self._make_fully_connected(in_features=256 * 2 * 2, out_features=512),\n",
    "            self._make_fully_connected(in_features=512, out_features=512),\n",
    "            self._make_fully_connected(in_features=512, out_features=n_classes, last=True)\n",
    "        ]\n",
    "        super().__init__(*layers)\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight, 0, 0.01)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "def count_parameters(model):\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        total += np.prod(list(p.shape))\n",
    "    return total\n",
    "                \n",
    "vgg = VGGNetwork()\n",
    "print(vgg)\n",
    "print(\"Total parameters: {}\".format(count_parameters(vgg)))\n",
    "\n",
    "seminar.check_vgg(VGGNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся оптимизацией с моментом Нестерова. Будем использовать момент, равный 0.9 и начальный шаг обучения 0.01.\n",
    "\n",
    "Шпаргалка:\n",
    "\n",
    "```python\n",
    "torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
    "\n",
    "torch.optim.SGD(params, lr, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loss():\n",
    "    # Ваш код, который создает loss.\n",
    "    \n",
    "    loss = ...\n",
    "    \n",
    "    #\n",
    "    return loss\n",
    "\n",
    "def make_optimizer(model):\n",
    "    # Ваш код, который создает оптимизатор.\n",
    "    \n",
    "    optimizer = ...\n",
    "    \n",
    "    # Конец вашего кода.\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "seminar.check_loss_fn(make_loss)\n",
    "seminar.check_optimizer_fn(make_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем экспоненциально затухающий learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr(step):\n",
    "    return 0.1 + 0.9 * (0.998) ** step\n",
    "\n",
    "def make_lr_scheduler(optimizer):\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr)\n",
    "\n",
    "xs = np.linspace(0, 2000)\n",
    "print(\"График изменения learning rate:\")\n",
    "plt.plot(xs, 0.01 * lr(xs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # Разрешить интерактивные графики.\n",
    "    %matplotlib notebook\n",
    "    seminar.train_model(vgg, make_loss, make_optimizer, trainset, testset,\n",
    "                        lr_scheduler_fn=make_lr_scheduler,\n",
    "                        **TRAIN_KWARGS)\n",
    "    # Отключить интерактивные графики.\n",
    "    %matplotlib inline\n",
    "else:\n",
    "    %matplotlib inline\n",
    "    cvmade.plot.show_image(\"resources/01-simple-train.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batchnorm\n",
    "\n",
    "Batchnorm ( https://arxiv.org/pdf/1502.03167.pdf ) это техника улучшения сходимости сетей с большим количеством слоёв.\n",
    "\n",
    "Во время тренировки:\n",
    "\n",
    "<img src=\"resources/batchnorm.jpg\" align=\"left\" width=\"40%\" height=\"40%\"/> \n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "Во время применения статистики по батчу заменяются на более точные статистики по корпусу данных.\n",
    "\n",
    "Шпаргалка:\n",
    "\n",
    "```python\n",
    "torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VGGNetworkBN(VGGNetwork):\n",
    "    @staticmethod\n",
    "    def _make_conv3x3(in_channels, out_channels):\n",
    "        # Ваш код, который создает слои сети.\n",
    "        \n",
    "        conv_layer = ...\n",
    "        batchnorm_layer = ...\n",
    "        \n",
    "        # Конец вашего кода.\n",
    "        return torch.nn.Sequential(conv_layer, batchnorm_layer)\n",
    "    \n",
    "vgg_bn = VGGNetworkBN()\n",
    "print(vgg_bn)\n",
    "seminar.check_vgg_bn(VGGNetworkBN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # Разрешить интерактивные графики.\n",
    "    %matplotlib notebook\n",
    "    seminar.train_model(vgg_bn, make_loss, make_optimizer, trainset, testset,\n",
    "                        lr_scheduler_fn=make_lr_scheduler,\n",
    "                        **TRAIN_KWARGS)\n",
    "    # Отключить интерактивные графики.\n",
    "    %matplotlib inline\n",
    "else:\n",
    "    %matplotlib inline\n",
    "    cvmade.plot.show_image(\"resources/02-batchnorm.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Переобучение\n",
    "\n",
    "Из соображений производительности, мы используем небольшую сеть. На CIFAR10 она не переобучается. Чтобы воспроизвести ситуацию с переобучением, мы уменьшим количество тренировочных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SUBSET_SIZE = 5000\n",
    "\n",
    "trainset_small_indices = random.sample(list(range(len(trainset))), TRAIN_SUBSET_SIZE)\n",
    "trainset_small = torch.utils.data.Subset(trainset, trainset_small_indices)\n",
    "print(\"Small trainset size:\", len(trainset_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # Разрешить интерактивные графики.\n",
    "    %matplotlib notebook\n",
    "    seminar.train_model(vgg, make_loss, make_optimizer, trainset_small, testset,\n",
    "                        lr_scheduler_fn=make_lr_scheduler,\n",
    "                        **TRAIN_KWARGS)\n",
    "    # Отключить интерактивные графики.\n",
    "    %matplotlib inline\n",
    "else:\n",
    "    %matplotlib inline\n",
    "    cvmade.plot.show_image(\"resources/03-overfit.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 - регуляризация (Тихонова)\n",
    "\n",
    "К функции потерь добавляется сумма квадратов параметров модели с некотрым весом (weight decay). В итоге модель штрафуется за слишком большие веса модели. Ограничение на диапазон значений параметров приводит к уменьшению переобучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "def make_optimizer_l2_reg(model):\n",
    "    return torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "if TRAIN:\n",
    "    # Разрешить интерактивные графики.\n",
    "    %matplotlib notebook\n",
    "    seminar.train_model(vgg, make_loss, make_optimizer_l2_reg, trainset_small, testset,\n",
    "                        lr_scheduler_fn=make_lr_scheduler,\n",
    "                        **TRAIN_KWARGS)\n",
    "    # Отключить интерактивные графики.\n",
    "    %matplotlib inline\n",
    "else:\n",
    "    %matplotlib inline\n",
    "    cvmade.plot.show_image(\"resources/04-l2.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "Свертки содержат мало параметров, они не переобучаются. По-этому, dropout обычно используют в полносвязных слоях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VGGNetworkDO(VGGNetwork):\n",
    "    @staticmethod\n",
    "    def _make_fully_connected(in_features, out_features, last=False):\n",
    "        layers = [torch.nn.Linear(in_features, out_features)]\n",
    "        if not last:\n",
    "            layers.append(torch.nn.Dropout(p=0.85))\n",
    "        return torch.nn.Sequential(*layers)\n",
    "    \n",
    "vgg_do = VGGNetworkDO()\n",
    "print(vgg_do)\n",
    "seminar.check_vgg_do(VGGNetworkDO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # Разрешить интерактивные графики.\n",
    "    %matplotlib notebook\n",
    "    seminar.train_model(vgg_do, make_loss, make_optimizer, trainset_small, testset,\n",
    "                        lr_scheduler_fn=make_lr_scheduler,\n",
    "                        **TRAIN_KWARGS)\n",
    "    # Отключить интерактивные графики.\n",
    "    %matplotlib inline\n",
    "else:\n",
    "    %matplotlib inline\n",
    "    cvmade.plot.show_image(\"resources/05-dropout.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аугментация\n",
    "\n",
    "Мы используем следующие аугментации:\n",
    "1. Rotation / Scale / Offset\n",
    "2. Random crop\n",
    "3. Brightness / Contrast\n",
    "4. Blur\n",
    "\n",
    "Шпаргалка:\n",
    "```python\n",
    "cv2.getRotationMatrix2D(center, angle, scale)  # center: (x, y), angle: градусы, scale: число.\n",
    "\n",
    "cv2.warpAffine(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineAugmenter(object):\n",
    "    def __init__(self, min_scale=0.9, max_scale=1.1, max_offset=0.1):\n",
    "        self._min_scale = min_scale\n",
    "        self._max_scale = max_scale\n",
    "        self._max_offset = max_offset\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        image = np.array(image)  # PIL -> Numpy.\n",
    "        h, w, c = image.shape\n",
    "        assert c == 3\n",
    "        \n",
    "        angle = random.random() * 90 - 45\n",
    "        scale = self._min_scale + random.random() * (self._max_scale - self._min_scale)\n",
    "        x_offset = random.randint(-int(self._max_offset * w), int(self._max_offset * w))\n",
    "        y_offset = random.randint(-int(self._max_offset * h), int(self._max_offset * h))\n",
    "        \n",
    "        # Ваш код, который преобразует изображение используя угол angle,\n",
    "        # масштаб scale и смещение (x_offset, y_offset). Фон предлагается закрасить серым цветом.\n",
    "        \n",
    "        # Начало вашего кода.\n",
    "        \n",
    "        new_image = ...\n",
    "        \n",
    "        # Конец вашего кода.\n",
    "        \n",
    "        return Image.fromarray(new_image)  # Numpy -> PIL.\n",
    "\n",
    "  \n",
    "%matplotlib inline\n",
    "seminar.show_augmenter_results(AffineAugmenter(), DATA_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример:\n",
    "<div style=\"clear:both;\"></div>\n",
    "<img style=\"float: left;\" src=\"resources/aug-00-affine.jpg\" width=\"40%\" height=\"40%\">\n",
    "<div style=\"clear:both;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "augmentation_transform = torchvision.transforms.Compose([\n",
    "    AffineAugmenter(),\n",
    "    transform\n",
    "])\n",
    "trainset_augmented_small = torch.utils.data.Subset(\n",
    "    CIFAR10(DATA_ROOT, train=True, download=True, transform=augmentation_transform),\n",
    "    trainset_small_indices)\n",
    "print(\"Trainset size:\", len(trainset_augmented_small))\n",
    "\n",
    "if TRAIN:\n",
    "    # Разрешить интерактивные графики.\n",
    "    %matplotlib notebook\n",
    "    seminar.train_model(vgg_bn, make_loss, make_optimizer, trainset_augmented_small, testset,\n",
    "                        lr_scheduler_fn=make_lr_scheduler,\n",
    "                        **TRAIN_KWARGS)\n",
    "    # Отключить интерактивные графики.\n",
    "    %matplotlib inline\n",
    "else:\n",
    "    %matplotlib inline\n",
    "    cvmade.plot.show_image(\"resources/07-aug-affine.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шпаргалка:\n",
    "```(python)\n",
    "cv2.resize(src, dsize[, dst[, fx[, fy[, interpolation]]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropAugmenter(object):\n",
    "    def __init__(self, min_scale=0.8):\n",
    "        self._min_scale = min_scale\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        image = np.array(image)  # PIL -> Numpy.\n",
    "        h, w, c = image.shape\n",
    "        assert c == 3\n",
    "        scale = self._min_scale + random.random() * (1 - self._min_scale)\n",
    "        new_w = int(scale * w)\n",
    "        new_h = int(scale * h)\n",
    "        x = random.randint(0, w - new_w)\n",
    "        y = random.randint(0, h - new_h)\n",
    "        \n",
    "        # Ваш код, который создает изображение new_image с фрагментом изображения image, \n",
    "        # который задается смещением (x, y) и размером (new_w, new_h).\n",
    "        \n",
    "        # Начало вашего кода.\n",
    "        \n",
    "        new_image = ...\n",
    "    \n",
    "        # Конец вашего кода.\n",
    "        \n",
    "        return Image.fromarray(new_image)  # Numpy -> PIL.\n",
    "\n",
    "%matplotlib inline\n",
    "seminar.show_augmenter_results(CropAugmenter(), DATA_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример:\n",
    "<div style=\"clear:both;\"></div>\n",
    "<img style=\"float: left;\" src=\"resources/aug-01-crop.jpg\" width=\"40%\" height=\"40%\">\n",
    "<div style=\"clear:both;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrightnessContrastAugmenter(object):\n",
    "    def __init__(self, brightness=0.3, contrast=0.3):\n",
    "        self._brightness = brightness\n",
    "        self._contrast = contrast\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        image = np.array(image)  # PIL -> Numpy.\n",
    "        h, w, c = image.shape\n",
    "        assert c == 3\n",
    "        brightness = 2 * (random.random() - 0.5) * self._brightness\n",
    "        contrast = 1 + 2 * (random.random() - 0.5) * self._contrast\n",
    "        \n",
    "        # Ваш код, который применяет яркость brightness и контраст contrast.\n",
    "        \n",
    "        new_image = ...\n",
    "        \n",
    "        # Конец вашего кода.\n",
    "        \n",
    "        assert new_image.dtype == np.uint8\n",
    "        return Image.fromarray(new_image)  # Numpy -> PIL.\n",
    "    \n",
    "%matplotlib inline\n",
    "seminar.show_augmenter_results(BrightnessContrastAugmenter(), DATA_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример:\n",
    "<div style=\"clear:both;\"></div>\n",
    "<img style=\"float: left;\" src=\"resources/aug-02-color.jpg\" width=\"40%\" height=\"40%\">\n",
    "<div style=\"clear:both;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шпаргалка:\n",
    "    \n",
    "```(python)\n",
    "cv2.GaussianBlur(src, ksize, sigmaX[, dst[, sigmaY[, borderType]]])  # ksize: (w, h), sigmaX: число.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlurAugmenter(object):\n",
    "    def __init__(self, max_kernel=5):\n",
    "        self._max_kernel = max_kernel\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        kernel = random.randint(0, self._max_kernel // 2) * 2 + 1\n",
    "        if kernel == 1:\n",
    "            return image\n",
    "        image = np.array(image)  # PIL -> Numpy.\n",
    "        h, w, c = image.shape\n",
    "        assert c == 3\n",
    "        \n",
    "        # Начало вашего кода.\n",
    "        \n",
    "        new_image = ...\n",
    "        \n",
    "        # Конец вашего кода.\n",
    "        return Image.fromarray(new_image)  # Numpy -> PIL.\n",
    "    \n",
    "%matplotlib inline\n",
    "seminar.show_augmenter_results(BlurAugmenter(), DATA_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример:\n",
    "<div style=\"clear:both;\"></div>\n",
    "<img style=\"float: left;\" src=\"resources/aug-03-blur.jpg\" width=\"40%\" height=\"40%\">\n",
    "<div style=\"clear:both;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAugmentation(object):\n",
    "    def __init__(self, *augmenters):\n",
    "        self._augmenters = list(augmenters)\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        augmenter = random.choice(self._augmenters)\n",
    "        return augmenter(image)\n",
    "    \n",
    "augmenter = RandomAugmentation(AffineAugmenter(),\n",
    "                               CropAugmenter(),\n",
    "                               BrightnessContrastAugmenter(),\n",
    "                               BlurAugmenter())\n",
    "%matplotlib inline\n",
    "seminar.show_augmenter_results(augmenter, DATA_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример:\n",
    "<div style=\"clear:both;\"></div>\n",
    "<img style=\"float: left;\" src=\"resources/aug-04-random.jpg\" width=\"40%\" height=\"40%\">\n",
    "<div style=\"clear:both;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_transform = torchvision.transforms.Compose([\n",
    "    augmenter,\n",
    "    transform\n",
    "])\n",
    "trainset_augmented_small = torch.utils.data.Subset(\n",
    "    CIFAR10(DATA_ROOT, train=True, download=True, transform=augmentation_transform),\n",
    "    trainset_small_indices)\n",
    "print(\"Trainset size:\", len(trainset_augmented_small))\n",
    "\n",
    "if TRAIN:\n",
    "    # Разрешить интерактивные графики.\n",
    "    %matplotlib notebook\n",
    "    seminar.train_model(vgg_bn, make_loss, make_optimizer, trainset_augmented_small, testset,\n",
    "                        lr_scheduler_fn=make_lr_scheduler,\n",
    "                        **TRAIN_KWARGS)\n",
    "    # Отключить интерактивные графики.\n",
    "    %matplotlib inline\n",
    "else:\n",
    "    %matplotlib inline\n",
    "    cvmade.plot.show_image(\"resources/08-aug-all.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

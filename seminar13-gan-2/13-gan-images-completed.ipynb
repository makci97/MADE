{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MADE](resources/made.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Академия MADE\n",
    "\n",
    "\n",
    "# Семинар 13: GAN для генерации лиц\n",
    "\n",
    "Сегодня мы займемся задачей генерации лиц с помощью генеративно-состязательных сетей (GAN). Мы не будем применять никаких эвристик, связанных именно с лицами, поэтому полученный в итоге пайплайн можно будет использовать для генерации объектов иной природы, подставив нужный датасет (ну и, скорее всего, подправив какие-то из гиперпараметров).\n",
    "\n",
    "#### **План**:\n",
    "1. **GAN & картинки: Deep Convolutional GAN (DCGAN)**\n",
    "2. **(Baseline) DCGAN & BCELoss.**\n",
    "3. **(Wasserstein) Wasserstein distance & Gradient Penalty.**\n",
    "4. **(Advanced) Spectral Normalization, Self-Attention, TTUR.**\n",
    "5. **Анализ проблем и что делать дальше**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GAN & картинки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Tiny recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Overview](resources/gan_overview.png)\n",
    "\n",
    "[отсюда](https://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В базовом случае ([Goodfellow et al, 2014](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)) схема обучения GAN такова:\n",
    "- Есть две модели, генератор $G$ и дискриминатор $D$.\n",
    "- Генератор: \n",
    "  - На вход получает вектор шума $z$,\n",
    "  - На выходе показывает объект (например, картинку) $G(z)$.\n",
    "\n",
    "- Дискриминатор: \n",
    "  - На вход получает либо настоящий, либо сгенерированный объект ($x$, $G(z)$),\n",
    "  - На выходе отдает уверенность в том, что объект - настоящий ($D(x)$, $D(G(z))$).\n",
    "\n",
    "Обучение генератора и дискриминатора производится отдельными шагами:\n",
    "- \"Ошибка дискриминатора\" = **BCELoss**\n",
    "- На шаге обучения генератора ошибка дискриминатора **максимизируется** (градиентный подъем),\n",
    "- На шаге обучения дискриминатора ошибка дискриминатора **минимизируется** (good old градиентный спуск).\n",
    "\n",
    "На лекциях было показано, что при такой схеме обучения должно происходить \"сближение\" двух распределений - \"настоящего\" $p_{data}(x)$ и \"сгенерированного\" $p_{g}(x)$ - с дивергенцией Йенсена-Шэннона в качестве критерия близости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Засовываем картинки в GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы оригинальной статьи использовали в экспериментах только полносвязные сети, в том числе и для генерации изображений (в низком разрешении).\n",
    "Как мы знаем, там, где нужна работа с картинками, ~~полносвязным сетям места нет~~ обычно более эффективными оказываются сверточные сети. Одной из первых публикаций по теме использования сверточных сетей для генерации изображений с помощью GAN была статья [\"Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks\"](https://arxiv.org/pdf/1511.06434.pdf). Основные тезисы:\n",
    "* Не использовать слои \"грубого\" изменения размера карт активаций (`Pooling`, `Upsampling`); использовать сверточные слои (со `stride`> 1) и слои с транспонированными свертками,\n",
    "* Использовать `BatchNorm` в обеих моделях,\n",
    "* Не использовать полносвязные слои вообще,\n",
    "* В генераторе использовать `ReLU` и `Tanh` (в конце),\n",
    "* В дискриминаторе использовать `LeakyReLU`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dcgan](resources/dcgan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, один из соавторов статьи про `DCGAN` и ключевая фигура в разработке `PyTorch` Soumith Chintala выложил [свои рекомендации](https://github.com/soumith/ganhacks) по обучению GAN. Некоторые из них мы используем в своих экспериментах, а именно:\n",
    "- Не использовать \"смешанные\" батчи (из настоящих и сгенерированных изображений), делать инференс по-отдельности,\n",
    "- Использовать \"soft labels\" (`0+eps` вместо `0`, `1.0-eps` вместо `1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее соберем архитектуру, подобную `DCGAN`, обучим ее с небольшими изменениями и посмотрим, что из этого получится."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (Baseline) DCGAN & BCELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При установке `TRAIN` = `False` вместо обучения будут подгружаться веса модели и ожидаемые результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем:\n",
    "1. Подгрузку данных\n",
    "2. Классы для дискриминатора и генератора\n",
    "3. Функцию для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_device, reproduce, show_data_batch\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import autograd\n",
    "\n",
    "from torch.nn import Parameter as P\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproduce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Надо скачать [выровненные изображения лиц](https://drive.google.com/file/d/0B7EVK8r0v71pZjFTYXZWM3FlRnM/view?usp=sharing) из датасета [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). Затем распаковать и положить в папку `DATA_ROOT`.\n",
    "\n",
    "*Параметры и данные из п.2.1. будем использовать во всех дальнейших экспериментах без изменений.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"./data/\"\n",
    "IMAGE_SIZE = 64\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 16\n",
    "NUM_TO_SHOW = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс `ImageFolder` умеет доставать картинки из подпапок корневой директории, при этом считая отдельный папки отдельными классами. Нам метки классов не понадобятся, не забудем учесть это при получении батчей из загрузчика данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(root=DATA_ROOT,\n",
    "                                           transform=transforms.Compose([\n",
    "                                               transforms.Resize(IMAGE_SIZE),\n",
    "                                               transforms.CenterCrop(IMAGE_SIZE),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                           ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,\n",
    "                                         drop_last=True, pin_memory=True)\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "show_data_batch(batch, max_images=NUM_TO_SHOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим гиперпараметры моделей генератора и дискриминатора.\n",
    "* `LATENT_DIM`: размерность (длина) вектора шума, из которого мы будем получать лица с помощью генератора,\n",
    "* `IMAGE_CHANNELS`: число каналов в изображениях (если захочется генерировать grayscale-изображения, нужно поставить `=1`),\n",
    "* `*_BASE_FEATURES`: параметры, определяющие начальнуе глубину сверточных слоев в обеих моделях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 100\n",
    "IMAGE_CHANNELS = 3\n",
    "DISCRIMINATOR_BASE_FEATURES = 64\n",
    "GENERATOR_BASE_FEATURES = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Дискриминатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с дискриминатора. Учтя рекомендациям авторов `DCGAN`, его структуру сделаем следующей:\n",
    "* На вход подается изображение с заданным числом каналов (`input_channels`);\n",
    "* Тело состоит из последовательных блоков вида `Conv2d - BN2d  - LeakyReLU`; в первом блоке опущен `BN2d` (**почему?**), в последнем есть только `Conv2d`;\n",
    "* Сверточные слои сделаем со `stride=2`, т.к. мы не хотим использовать `MaxPool2d`; `kernel_size=4`, `padding=1`;\n",
    "* Добавим параметр `with_sigmoid` на будущее (понадобится, когда перейдем от кросс-энтропии к другому критерию)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBasic(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_channels=IMAGE_CHANNELS, \n",
    "                 base_num_features=DISCRIMINATOR_BASE_FEATURES, \n",
    "                 with_sigmoid=True):\n",
    "        super(DiscriminatorBasic, self).__init__()\n",
    "        layers = [\n",
    "            # input size: input_channels x 64 x 64\n",
    "            \n",
    "            nn.Conv2d(input_channels, base_num_features, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: base_num_features x 32 x 32\n",
    "\n",
    "            nn.Conv2d(base_num_features, base_num_features * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_num_features * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (base_num_features * 2) x 16 x 16\n",
    "\n",
    "            nn.Conv2d(base_num_features * 2, base_num_features * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_num_features * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (base_num_features * 4) x 8 x 8\n",
    "\n",
    "            nn.Conv2d(base_num_features * 4, base_num_features * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_num_features * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (base_num_features * 8) x 4 x 4\n",
    "\n",
    "            nn.Conv2d(base_num_features * 8, 1, 4, 1, 0, bias=False),\n",
    "            # state size: 1 x 1 x 1\n",
    "        ]\n",
    "        \n",
    "        if with_sigmoid:\n",
    "            layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.main(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = DiscriminatorBasic()\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели рассчитаны на работу с изображениями `64x64` (размер уменьшается в 64 раза); проверим, что на выходе получается одно-единственное число (уверенность дискриминатора в том, что пример - реальный):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, IMAGE_CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "y = discriminator(x)\n",
    "assert y.size() == (4, 1, 1, 1), y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Генератор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С генератором чуть сложнее: на вход он получает вектор шума `z` длины `LATENT_DIM`, но в следующем виде: `LATENT_DIM x 1 x 1`.\n",
    "В отличие от дискриминатора, здесь используем транспонированные свертки, активации `ReLU` в середине и `Tanh` в самом конце."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorBasic(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_channels=LATENT_DIM, \n",
    "                 base_num_features=GENERATOR_BASE_FEATURES, \n",
    "                 output_channels=IMAGE_CHANNELS):\n",
    "        super(GeneratorBasic, self).__init__()\n",
    "        layers = [\n",
    "            # input is Z, going into a convolution\n",
    "            \n",
    "            nn.ConvTranspose2d(input_channels, base_num_features * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(base_num_features * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size: (base_num_features * 8) x 4 x 4\n",
    "\n",
    "            nn.ConvTranspose2d(base_num_features * 8, base_num_features * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_num_features * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size: (base_num_features * 4) x 8 x 8\n",
    "\n",
    "            nn.ConvTranspose2d(base_num_features * 4, base_num_features * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_num_features * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size: (base_num_features * 2) x 16 x 16\n",
    "\n",
    "            nn.ConvTranspose2d(base_num_features * 2, base_num_features, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_num_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size: (base_num_features) x 32 x 32\n",
    "\n",
    "            nn.ConvTranspose2d(base_num_features, output_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size: output_channels x 64 x 64\n",
    "        ]\n",
    "\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.main(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GeneratorBasic()\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генератор тоже заточен под один размер изображений - `64x64`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, LATENT_DIM, 1, 1)\n",
    "y = generator(x)\n",
    "assert y.size() == (4, IMAGE_CHANNELS, IMAGE_SIZE, IMAGE_SIZE), y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем также функцию для ручной инициализации весов наших моделей (**как инициализируются веса по умолчанию?**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m, scale=0.02):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, scale)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, scale)\n",
    "        torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим фиксированный вектор шума, чтобы отслеживать успехи генератора по мере обучения GAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print(device)\n",
    "\n",
    "FIXED_NOISE = torch.randn(NUM_TO_SHOW, LATENT_DIM, 1, 1, device=device)\n",
    "print(FIXED_NOISE.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Код для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем `smooth labels` и [рекомендованные](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) параметры оптимизатора (`Adam`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_LABEL = 0.9  # better use 1 - random(0.0, 0.1)\n",
    "FAKE_LABEL = 0.1  # better use random(0.0, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдем к написанию функции, которая будет получать на вход модели/оптимизаторы/данные/прочие параметры и выполнять 1 эпоху обучения. Эту функцию мы напишем несколько раз по мере усложнения пайплайна обучения.\n",
    "\n",
    "Напомним алгоритм обучения GAN в [первозданном](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) виде:\n",
    "\n",
    "![Algo](resources/gan_algo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_basic(generator, discriminator, \n",
    "                      optimizer_generator, optimizer_discriminator,\n",
    "                      dataloader, epoch, num_epochs, device,\n",
    "                      criterion):\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    # Logging routine\n",
    "    generator_loss_list, discriminator_loss_list = [], []\n",
    "    discriminator_prob_real_list, discriminator_prob_fake_before_list, discriminator_prob_fake_after_list = [], [], []\n",
    "    generator_images_list = []\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ############################\n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "        ## Train with all-real batch\n",
    "        # Forward pass real batch through D\n",
    "        real_batch, _ = batch\n",
    "        output = discriminator(real_batch.to(device)).view(-1)\n",
    "        \n",
    "        # Calculate loss on all-real batch\n",
    "        label = torch.full((BATCH_SIZE,), REAL_LABEL, device=device)\n",
    "        discriminator_loss_real = criterion(output, label)\n",
    "        \n",
    "        # Calculate gradients for D in backward pass\n",
    "        discriminator_loss_real.backward()\n",
    "        \n",
    "        # Save for logging\n",
    "        discriminator_prob_real = output.mean().item()\n",
    "        discriminator_prob_real_list.append(discriminator_prob_real)\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(BATCH_SIZE, LATENT_DIM, 1, 1, device=device)\n",
    "        \n",
    "        # Generate fake image batch with G\n",
    "        fake_batch = generator(noise)\n",
    "        \n",
    "        # Classify all fake batch with D\n",
    "        output = discriminator(fake_batch.detach()).view(-1)  # Detach!\n",
    "        \n",
    "        # Save for logging\n",
    "        discriminator_prob_fake_before = output.mean().item()\n",
    "        discriminator_prob_fake_before_list.append(discriminator_prob_fake_before)\n",
    "        \n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        label.fill_(FAKE_LABEL)\n",
    "        discriminator_loss_fake = criterion(output, label)\n",
    "        \n",
    "        # Calculate the gradients for this batch\n",
    "        discriminator_loss_fake.backward()\n",
    "        \n",
    "        discriminator_loss = discriminator_loss_real + discriminator_loss_fake\n",
    "        \n",
    "        # Update D\n",
    "        optimizer_discriminator.step()\n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ############################\n",
    "        generator.zero_grad()\n",
    "        \n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = discriminator(fake_batch).view(-1)\n",
    "                \n",
    "        # Save for logging\n",
    "        discriminator_prob_fake_after = output.mean().item()\n",
    "        discriminator_prob_fake_after_list.append(discriminator_prob_fake_after)\n",
    "        \n",
    "        # Calculate G's loss based on this output\n",
    "        label.fill_(REAL_LABEL)  # fake labels are real for generator cost\n",
    "        generator_loss = criterion(output, label)\n",
    "        \n",
    "        # Calculate gradients for G\n",
    "        generator_loss.backward()\n",
    "        \n",
    "        # Update G\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 500 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     discriminator_loss.item(), generator_loss.item(), discriminator_prob_real,\n",
    "                     discriminator_prob_fake_before, discriminator_prob_fake_after))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        generator_loss_list.append(generator_loss.item())\n",
    "        discriminator_loss_list.append(discriminator_loss.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if i == len(dataloader) - 1:\n",
    "            generator.eval()\n",
    "            with torch.no_grad():\n",
    "                fake = generator(FIXED_NOISE).detach().cpu()\n",
    "            generator_images_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "            generator.train()\n",
    "\n",
    "    return {\"generator_loss_list\": generator_loss_list,\n",
    "            \"discriminator_loss_list\": discriminator_loss_list,\n",
    "            \"discriminator_prob_real_list\": discriminator_prob_real_list,\n",
    "            \"discriminator_prob_fake_before_list\": discriminator_prob_fake_before_list,\n",
    "            \"discriminator_prob_fake_after_list\": discriminator_prob_fake_after_list,\n",
    "            \"generator_images_list\": generator_images_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0002\n",
    "BETAS = [0.5, 0.999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GeneratorBasic()\n",
    "generator.apply(weights_init)\n",
    "optimizer_generator = optim.Adam(generator.parameters(), lr=LR, betas=BETAS, amsgrad=True)\n",
    "\n",
    "discriminator = DiscriminatorBasic()\n",
    "discriminator.apply(weights_init)\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=LR, betas=BETAS, amsgrad=True)\n",
    "\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {key: [] for key in [\"generator_loss_list\", \n",
    "                               \"discriminator_loss_list\", \n",
    "                               \"discriminator_prob_real_list\", \n",
    "                               \"discriminator_prob_fake_before_list\", \"discriminator_prob_fake_after_list\",\n",
    "                               \"generator_images_list\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    NUM_EPOCHS = 50\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_results = train_epoch_basic(generator, discriminator, \n",
    "                                          optimizer_generator, optimizer_discriminator,\n",
    "                                          dataloader, epoch, NUM_EPOCHS, device,\n",
    "                                          criterion)\n",
    "        for key in results:\n",
    "            results[key].extend(epoch_results[key])\n",
    "            \n",
    "    with open(\"./cached/results_basic.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(results, fp)\n",
    "    with open(\"./weights/checkpoint_basic.pth.tar\", \"wb\") as fp:\n",
    "        torch.save({\"generator\": generator.state_dict(),\n",
    "                    \"discriminator\": discriminator.state_dict()}, fp)\n",
    "else:\n",
    "    with open(\"./cached/training_basic.txt\", \"rt\") as fp:\n",
    "        for line in fp:\n",
    "            print(line.strip())\n",
    "    with open(\"./cached/results_basic.pkl\", \"rb\") as fp:\n",
    "        results = pickle.load(fp)\n",
    "    with open(\"./weights/checkpoint_basic.pth.tar\", \"rb\") as fp:\n",
    "        states = torch.load(fp, map_location=\"cpu\")\n",
    "        generator.load_state_dict(states[\"generator\"])\n",
    "        discriminator.load_state_dict(states[\"discriminator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(results[\"generator_loss_list\"], label=\"G\", alpha=0.5)\n",
    "plt.plot(results[\"discriminator_loss_list\"], label=\"D\", alpha=0.5)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "# plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "plt.axis(\"off\")\n",
    "imgs = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in results[\"generator_images_list\"][::5]]\n",
    "ani = animation.ArtistAnimation(fig, imgs, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (Wasserstein) Wasserstein distance & Gradient Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что использованная нами только что схема обучения (с кросс-энтропией) была подвергнута справедливой критике [не](https://arxiv.org/pdf/1701.07875) [раз](https://arxiv.org/pdf/1611.04076). В качестве альтернативы $D_{js}$ как метрики близости двух распределений предлагается, например, использовать [аппроксимацию расстояния Васерштейна](https://arxiv.org/abs/1701.07875) (оно же `Earth Mover Distance`):\n",
    "\n",
    "![Wdist](resources/wasserstein_loss.png)\n",
    "\n",
    "Это, в свою очередь, приводит к следующему алгоритму обучения:\n",
    "\n",
    "![Algo](resources/wgan_algo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно отметить, что для работы в этой схеме появляются ограничения на функцию, вычисляемую дискриминатором: она должна быть ограниченной по Липшицу. Для этого в оригинальной статье предлагается довольно грубый метод - усечения весов дискриминатора (см. п.7 алгоритма выше). В [статье](https://arxiv.org/pdf/1704.00028.pdf), развивающей идеи Wasserstein GAN, предложен альтернативный подход к ограничиванию константы Липшица для дискриминатора - это непосредственное добавление в функцию потерь нормы разности градиентов весов дискриминатора и единицы. Мы попробуем только второй подход."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Wasserstein GAN + Gradient Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот подход описан в статье [Improved Training of Wasserstein GANs](https://arxiv.org/pdf/1704.00028.pdf), где утверждается, что при использовании Gradient penalty генератор лучше аппроксимирует мультимодальные распределения, а обучение становится более стабильным.\n",
    "Итоговый критерий выглядит так:\n",
    "\n",
    "![gp](resources/wgan_gradient_penalty.png)\n",
    "\n",
    "Таким образом, нам понадобится дополнительно реализовать функцию для вычисления нормы градиентов весов дискриминатора и поместить ее в соответствующий лосс.\n",
    "Взято [отсюда](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan_gp/wgan_gp.py). \n",
    "\n",
    "Обратите внимание, на каких примерах считаются нормы градиентов - это результаты линейной интерполяции между сгенерированными и реальными изображениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_batch, fake_batch, device):\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.Tensor(np.random.random((real_batch.size(0), 1, 1, 1))).to(device)\n",
    "    \n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_batch + ((1 - alpha) * fake_batch)).requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates).cpu()\n",
    "    \n",
    "    # Get gradient w.r.t. interpolates\n",
    "    fake = torch.ones(real_batch.shape[0], 1, 1, 1).float().requires_grad_(False)\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    \n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_wasserstein(generator, discriminator, \n",
    "                            optimizer_generator, optimizer_discriminator,\n",
    "                            dataloader, epoch, num_epochs, device,\n",
    "                            n_critic, gradient_penalty_lambda):\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    generator_loss_list, discriminator_loss_list = [], []\n",
    "    discriminator_gradient_penalty_list = []\n",
    "    generator_images_list = []\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ############################\n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "        real_batch = batch[0].to(device)\n",
    "        output_real = discriminator(real_batch)\n",
    "        \n",
    "        noise = torch.randn(BATCH_SIZE, LATENT_DIM, 1, 1, device=device)\n",
    "        fake_batch = generator(noise)\n",
    "        output_fake = discriminator(fake_batch)\n",
    "        \n",
    "        discriminator_loss = - torch.mean(output_real) + torch.mean(output_fake)\n",
    "        discriminator_loss_list.append(discriminator_loss.item())\n",
    "        \n",
    "        if gradient_penalty_lambda > 0:\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_batch, fake_batch, device)\n",
    "            discriminator_gradient_penalty_list.append(gradient_penalty.item())\n",
    "\n",
    "            discriminator_loss += gradient_penalty_lambda * gradient_penalty\n",
    "        else:\n",
    "            gradient_penalty = torch.zeros(1)\n",
    "        \n",
    "        discriminator_loss.backward()\n",
    "        optimizer_discriminator.step()\n",
    "        \n",
    "\n",
    "        if i % n_critic == 0:\n",
    "                \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ############################\n",
    "            generator.zero_grad()\n",
    "            \n",
    "            noise = torch.randn(BATCH_SIZE, LATENT_DIM, 1, 1, device=device)\n",
    "            fake_batch = generator(noise)\n",
    "            output_fake = discriminator(fake_batch)\n",
    "            \n",
    "            output = discriminator(fake_batch)\n",
    "            generator_loss = - torch.mean(output_fake)\n",
    "            generator_loss.backward()\n",
    "            optimizer_generator.step()\n",
    "        \n",
    "        generator_loss_list.append(generator_loss.item())\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 500 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tGP: %.4f' %\n",
    "                  (epoch, num_epochs, i, len(dataloader), \n",
    "                   discriminator_loss.item(), generator_loss.item(), gradient_penalty.item()))\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if i == len(dataloader) - 1:\n",
    "            with torch.no_grad():\n",
    "                fake = generator(FIXED_NOISE).detach().cpu()\n",
    "            generator_images_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "    return {\"generator_loss_list\": generator_loss_list,\n",
    "            \"discriminator_loss_list\": discriminator_loss_list,\n",
    "            \"discriminator_gradient_penalty_list\": discriminator_gradient_penalty_list,\n",
    "            \"generator_images_list\": generator_images_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADIENT_PENALTY_LAMBDA = 10\n",
    "\n",
    "N_CRITIC = 5\n",
    "LR = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку в `Wasserstein GAN` дискриминатор не является бинарным классификатором (а служит функцией, определяющей расстояние между двумя распределениями), оборачивать его выход нелинейностью не нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GeneratorBasic()\n",
    "generator.apply(weights_init)\n",
    "optimizer_generator = optim.RMSprop(generator.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "discriminator = DiscriminatorBasic(with_sigmoid=False)\n",
    "discriminator.apply(weights_init)\n",
    "optimizer_discriminator = optim.RMSprop(discriminator.parameters(), lr=LR)\n",
    "\n",
    "criterion = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {key: [] for key in [\"generator_loss_list\", \n",
    "                               \"discriminator_loss_list\", \n",
    "                               \"discriminator_gradient_penalty_list\",\n",
    "                               \"generator_images_list\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    NUM_EPOCHS = 50\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_results = train_epoch_wasserstein(generator, discriminator,\n",
    "                                                optimizer_generator, optimizer_discriminator,\n",
    "                                                dataloader, epoch, NUM_EPOCHS, device,\n",
    "                                                N_CRITIC, GRADIENT_PENALTY_LAMBDA)\n",
    "        for key in results:\n",
    "            results[key].extend(epoch_results[key])\n",
    "        \n",
    "    with open(\"./cached/results_wasserstein.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(results, fp)\n",
    "    with open(\"./weights/checkpoint_wasserstein.pth.tar\", \"wb\") as fp:\n",
    "        torch.save({\"generator\": generator.state_dict(),\n",
    "                    \"discriminator\": discriminator.state_dict()}, fp)\n",
    "else:\n",
    "    with open(\"./cached/training_wasserstein.txt\", \"rt\") as fp:\n",
    "        for line in fp:\n",
    "            print(line.strip())\n",
    "    with open(\"./cached/results_wasserstein.pkl\", \"rb\") as fp:\n",
    "        results = pickle.load(fp)\n",
    "    with open(\"./weights/checkpoint_wasserstein.pth.tar\", \"rb\") as fp:\n",
    "        states = torch.load(fp, map_location=\"cpu\")\n",
    "        generator.load_state_dict(states[\"generator\"])\n",
    "        discriminator.load_state_dict(states[\"discriminator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(results[\"generator_loss_list\"], label=\"G\", alpha=0.5)\n",
    "plt.plot(results[\"discriminator_loss_list\"], label=\"D\", alpha=0.5)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "plt.axis(\"off\")\n",
    "imgs = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in results[\"generator_images_list\"][::5]]\n",
    "ani = animation.ArtistAnimation(fig, imgs, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (Advanced) Spectral Normalization, Self-Attention, TTUR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расширим теперь возможности наши модели, следуя некоторым идеям из статьи [SAGAN](https://arxiv.org/pdf/1805.08318.pdf):\n",
    "* Во-первых, добавим в модели слои `Self-Attention`,\n",
    "* Во-вторых, добавим в модели поддержку спектральной нормализации весов,\n",
    "* В-третьих, будем использовать разные `LR` для генератора и дискриминатора (но обновлять обе модели с одной частотой)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы отмечают, что очень важным аспектом генерации изображений является рецептивное поле нейронов в сверточных слоях. В самом деле, поскольку архитектуры наподобие `DCGAN` не используют полносвязные слои (по отдельным причинам), то может оказаться так, что нейроны последних слоев дискриминатора будут видеть только часть изображения. Это, в свою очередь, может позволять генератору выдавать изображения собак с 5 ногами и все в таком духе.\n",
    "\n",
    "В качестве решения они предлагают использовать слой `Self-Attention`, который позволяет \"передавать\" конкретному нейрону, видящему только \"свою\" область, информацию обо всех других областях. При этом нейрон сам \"решает\", насколько важным для него является каждая конкретная область, поскольку используется механизм взвешивания.\n",
    "\n",
    "![SA](resources/self_attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализацию возьмем из [репозитория](https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py) BigGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ch, with_sn=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        # Channel multiplier\n",
    "        self.ch = ch\n",
    "        self.conv2d = SNConv2d if with_sn else nn.Conv2d\n",
    "        self.theta = self.conv2d(self.ch, self.ch // 8, kernel_size=1, padding=0, bias=False)\n",
    "        self.phi = self.conv2d(self.ch, self.ch // 8, kernel_size=1, padding=0, bias=False)\n",
    "        self.g = self.conv2d(self.ch, self.ch // 2, kernel_size=1, padding=0, bias=False)\n",
    "        self.o = self.conv2d(self.ch // 2, self.ch, kernel_size=1, padding=0, bias=False)\n",
    "        # Learnable gain parameter\n",
    "        self.gamma = P(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # Apply convs\n",
    "        theta = self.theta(x)\n",
    "        phi = F.max_pool2d(self.phi(x), [2, 2])\n",
    "        g = F.max_pool2d(self.g(x), [2, 2])\n",
    "        # Perform reshapes\n",
    "        theta = theta.view(-1, self.ch // 8, x.shape[2] * x.shape[3])\n",
    "        phi = phi.view(-1, self.ch // 8, x.shape[2] * x.shape[3] // 4)\n",
    "        g = g.view(-1, self.ch // 2, x.shape[2] * x.shape[3] // 4)\n",
    "        # Matmul and softmax to get attention maps\n",
    "        beta = F.softmax(torch.bmm(theta.transpose(1, 2), phi), -1)\n",
    "        # Attention map times g path\n",
    "        o = self.o(torch.bmm(g, beta.transpose(1, 2)).view(-1, self.ch // 2, x.shape[2], x.shape[3]))\n",
    "        return self.gamma * o + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Spectral Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В работе по `Wasserstein GAN` было показано, что для его стабильной сходимости необходимо накладывать ограничения на дискриминатор. Впрочем, оказалось, что ограниченность дискриминатора помогает и в более общих случаях (при других функционалах). Авторы статьи [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957) предложили применять для ограничения дискриминатора спектральную нормализацию весов, что, по задумке, должно приводить к константе Липшица ~ 1. Но если в случае с методом `Gradient Penalty` (использованном нами выше) ограничивание каких-то данных (на которых считается градиент и его норма), то спектральная нормализация данных не требует. \n",
    "\n",
    "В статье `SAGAN` авторы использовали эту нормализацию и в генераторе тоже. Мы попробуем вариант с нормализацией только в дискриминатор, самостоятельно можете попробовать и другие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectral import SNConv2d, SNConvTranspose2d  # тоже из BigGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorAdvanced(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels=IMAGE_CHANNELS, base_num_features=DISCRIMINATOR_BASE_FEATURES, with_sn=False):\n",
    "        super(DiscriminatorAdvanced, self).__init__()\n",
    "        conv2d = SNConv2d if with_sn else nn.Conv2d\n",
    "        layers = [\n",
    "            # input size: input_channels x 64 x 64\n",
    "            \n",
    "            conv2d(input_channels, base_num_features, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: base_num_features x 32 x 32\n",
    "\n",
    "            conv2d(base_num_features, base_num_features * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_num_features * 2) if not with_sn else nn.Identity(),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            SelfAttention(base_num_features * 2, with_sn=with_sn),\n",
    "            # state size: (base_num_features * 2) x 16 x 16\n",
    "\n",
    "            conv2d(base_num_features * 2, base_num_features * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_num_features * 4) if not with_sn else nn.Identity(),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (base_num_features * 4) x 8 x 8\n",
    "\n",
    "            conv2d(base_num_features * 4, base_num_features * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_num_features * 8) if not with_sn else nn.Identity(),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            SelfAttention(base_num_features * 8, with_sn=with_sn),\n",
    "            # state size: (base_num_features * 8) x 4 x 4\n",
    "\n",
    "            conv2d(base_num_features * 8, 1, 4, 1, 0, bias=False),\n",
    "            # state size: 1 x 4 x 4\n",
    "        ]\n",
    "        \n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.main(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = DiscriminatorAdvanced(with_sn=True)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, IMAGE_CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "y = discriminator(x)\n",
    "assert y.size() == (4, 1, 1, 1), y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorAdvanced(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels=LATENT_DIM, base_num_features=GENERATOR_BASE_FEATURES, output_channels=IMAGE_CHANNELS, with_sn=False):\n",
    "        super(GeneratorAdvanced, self).__init__()\n",
    "        self.conv2d_t = SNConvTranspose2d if with_sn else nn.ConvTranspose2d\n",
    "        layers = [\n",
    "            # input is Z, going into a convolution\n",
    "            \n",
    "            self.conv2d_t(input_channels, base_num_features * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(base_num_features * 8) if not with_sn else nn.Identity(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size: (base_num_features * 8) x 4 x 4\n",
    "\n",
    "            self.conv2d_t(base_num_features * 8, base_num_features * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_num_features * 4) if not with_sn else nn.Identity(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SelfAttention(base_num_features * 4, with_sn=with_sn),\n",
    "            # state size: (base_num_features * 4) x 8 x 8\n",
    "\n",
    "            self.conv2d_t(base_num_features * 4, base_num_features * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_num_features * 2) if not with_sn else nn.Identity(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size: (base_num_features * 2) x 16 x 16\n",
    "\n",
    "            self.conv2d_t(base_num_features * 2, base_num_features, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(base_num_features) if not with_sn else nn.Identity(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SelfAttention(base_num_features, with_sn=with_sn),\n",
    "            # state size: (base_num_features) x 32 x 32\n",
    "\n",
    "            self.conv2d_t(base_num_features, output_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size: output_channels x 64 x 64\n",
    "        ]\n",
    "\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.main(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GeneratorAdvanced(with_sn=False)\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, LATENT_DIM, 1, 1)\n",
    "y = generator(x)\n",
    "assert y.size() == (4, IMAGE_CHANNELS, IMAGE_SIZE, IMAGE_SIZE), y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдем к схеме, в которой дискриминатор и генератор обучаются с одной частотой (то есть `N_CRITIC = 1`), но выставим разные значения `LR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CRITIC = 1\n",
    "DISCRIMINATOR_LR = 0.0005\n",
    "GENERATOR_LR = 0.0001\n",
    "BETAS = [0.0, 0.999]\n",
    "GRADIENT_PENALTY_LAMBDA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GeneratorAdvanced(with_sn=False)\n",
    "generator.apply(weights_init)\n",
    "optimizer_generator = optim.Adam(generator.parameters(), lr=LR, betas=BETAS, amsgrad=True)\n",
    "\n",
    "discriminator = DiscriminatorAdvanced(with_sn=True)\n",
    "discriminator.apply(weights_init)\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=LR, betas=BETAS, amsgrad=True)\n",
    "\n",
    "criterion = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {key: [] for key in [\"generator_loss_list\", \n",
    "                               \"discriminator_loss_list\", \n",
    "                               \"discriminator_gradient_penalty_list\",\n",
    "                               \"generator_images_list\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    NUM_EPOCHS = 50\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_results = train_epoch_wasserstein(generator, discriminator,\n",
    "                                                optimizer_generator, optimizer_discriminator,\n",
    "                                                dataloader, epoch, NUM_EPOCHS, device,\n",
    "                                                N_CRITIC, GRADIENT_PENALTY_LAMBDA)\n",
    "        for key in results:\n",
    "            results[key].extend(epoch_results[key])\n",
    "        \n",
    "    with open(\"./cached/results_advanced.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(results, fp)\n",
    "    with open(\"./weights/checkpoint_advanced.pth.tar\", \"wb\") as fp:\n",
    "        torch.save({\"generator\": generator.state_dict(),\n",
    "                    \"discriminator\": discriminator.state_dict()}, fp)\n",
    "else:\n",
    "    with open(\"./cached/training_advanced.txt\", \"rt\") as fp:\n",
    "        for line in fp:\n",
    "            print(line.strip())\n",
    "    with open(\"./cached/results_advanced.pkl\", \"rb\") as fp:\n",
    "        results = pickle.load(fp)\n",
    "    with open(\"./weights/checkpoint_advanced.pth.tar\", \"rb\") as fp:\n",
    "        states = torch.load(fp, map_location=\"cpu\")\n",
    "        generator.load_state_dict(states[\"generator\"])\n",
    "        discriminator.load_state_dict(states[\"discriminator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(results[\"generator_loss_list\"], label=\"G\", alpha=0.5)\n",
    "plt.plot(results[\"discriminator_loss_list\"], label=\"D\", alpha=0.5)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "plt.axis(\"off\")\n",
    "imgs = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in results[\"generator_images_list\"][::5]]\n",
    "ani = animation.ArtistAnimation(fig, imgs, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Что делать дальше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы рассмотрели базовый подход к генерации изображений с помощью GAN и немного расширили его, заменив критерий обучения и добавив несколько компонентов для улучшения сходимости моделей. Однако, любая из упомянутых в семинаре статей (и тем более из неупомянутых) содержит еще много идей, как сделать генерацию лучше. В конце страницы вы найдете список статей (субъективный), к которым можно обратиться за идеями. А пока коротко перечислим, что можно сделать в продолжение семинара."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Во-первых, исследовать проблему коллапсирования мод в полученных моделях.** Невооруженным глазом видно, что некоторые из сгенерированных изображений очень похожи друг на друга. Попробуйте сгенерировать несколько лиц и отыскать для них ближайшие по обучающей выборке. \n",
    "\n",
    "**Во-вторых, исследовать \"гладкость\" представлений в генераторе.** Сгенерируйте пару случайных векторов $(z_1, z_2)$ и, сделав линейную интерполяцию между ними , насэмплируйте несколько новых векторов $z'=\\alpha z_1 + (1 - \\alpha) z_2$. Пропустите векторы через генератор и проверьте, насколько хорошо \"перетекают\" получаемые на выходе лица по мере движения от вектора $z_1$ к вектору $z_2$.\n",
    "\n",
    "**В-третьих, добавить больше реализованных идей из перечисленных в семинаре статей.** Мы не пробовали `Gradient Clipping` в `WGAN` (хотя сказали, что он должен быть хуже, чем `Gradient Penalty`), не добавляли спектральную нормализацию в генератор, не подбирали толком константы..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. GAN Readlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статьи:\n",
    "1. [Generative Adversarial Networks](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) (GAN)\n",
    "2. [Conditional GAN](https://arxiv.org/abs/1411.1784) (cGAN)\n",
    "3. [Unsupervised representation learning with deep convolutional generative adversarial networks](https://arxiv.org/pdf/1511.06434) (DCGAN)\n",
    "4. [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004) [Pix2Pix]\n",
    "5. [Wasserstein GAN](https://arxiv.org/abs/1701.07875) (Wasserstein GAN)\n",
    "6. [Improved Training of Wasserstein GANs](https://arxiv.org/abs/1704.00028) (Wasserstein GAN + Gradient Penalty)\n",
    "7. [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957) (SNGAN)\n",
    "8. [Self-Attention Generative Adverarial Networks](https://arxiv.org/pdf/1805.08318) (SAGAN)\n",
    "9. [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096) (BigGAN)\n",
    "10. [A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/abs/1812.04948) (StyleGAN)\n",
    "\n",
    "Репозитории и код:\n",
    "1. [PyTorch DCGAN example](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) (основа для этого семинара)\n",
    "2. [PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN) (простые реализации множества статей по GAN)\n",
    "3. [BigGAN](https://github.com/eriklindernoren/PyTorch-GAN) (реализация огромного числа фич для GAN)\n",
    "4. [BigGAN TF Hub Demo](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb) (ноутбук с предобученным BigGAN)\n",
    "5. [How to Train a GAN? Tips and tricks to make GANs work](https://github.com/soumith/ganhacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![interpolation](resources/interpolation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

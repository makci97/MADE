{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highlights detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохраняем кадры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_frame(img, size, interpolation=cv2.INTER_CUBIC):\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    if w < size and h < size:\n",
    "        return img\n",
    "\n",
    "    if (w <= h and w == size) or (h <= w and h == size):\n",
    "        return img\n",
    "    if w < h:\n",
    "        ow = size\n",
    "        oh = int(size * h / w)\n",
    "        output = cv2.resize(img, dsize=(ow, oh), interpolation=interpolation)\n",
    "    else:\n",
    "        oh = size\n",
    "        ow = int(size * w / h)\n",
    "        output = cv2.resize(img, dsize=(ow, oh), interpolation=interpolation)\n",
    "\n",
    "    if img.shape[2] == 1:\n",
    "        return output[:, :, np.newaxis]\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_frames_from_video(video_file, video_frames_dir, new_size=256, fps_factor=3):\n",
    "    video_capture = cv2.VideoCapture(video_file)\n",
    "    video_capture.set(cv2.CAP_PROP_POS_AVI_RATIO, 1)\n",
    "    video_len = video_capture.get(cv2.CAP_PROP_POS_MSEC) / 1000\n",
    "    video_capture.release()\n",
    "\n",
    "    video_capture = cv2.VideoCapture(video_file)\n",
    "\n",
    "    if not video_capture.isOpened():\n",
    "        return 0\n",
    "\n",
    "    frame_num = 0\n",
    "    frames_list = []\n",
    "\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    if fps == 1000 or fps == 0:\n",
    "        video_capture.release()\n",
    "        return 0\n",
    "\n",
    "    frame_rate = 1 / fps_factor\n",
    "    sec = 0\n",
    "\n",
    "    secs = []\n",
    "\n",
    "    while video_capture.isOpened():\n",
    "        video_capture.set(cv2.CAP_PROP_POS_MSEC, sec * 1000)\n",
    "\n",
    "        ret, frame = video_capture.read()\n",
    "\n",
    "        if ret:\n",
    "            frames_list.append(resize_frame(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), new_size))\n",
    "\n",
    "            frame_num += 1\n",
    "            secs.append(sec)\n",
    "\n",
    "            sec = sec + frame_rate\n",
    "            sec = round(sec, 2)\n",
    "\n",
    "            if sec > video_len:\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            if sec == video_len:\n",
    "                break\n",
    "\n",
    "            if sec < video_len:\n",
    "                if video_len - sec > 1:\n",
    "                    video_capture.release()\n",
    "                    return 0\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    video_capture.release()\n",
    "\n",
    "    os.mkdir(video_frames_dir)\n",
    "\n",
    "    frames_filename = os.path.join(video_frames_dir, 'frames')\n",
    "    np.savez_compressed(frames_filename, *frames_list)\n",
    "\n",
    "    frames_sec_filename = os.path.join(video_frames_dir, 'frames_timing.txt')\n",
    "    with open(frames_sec_filename, 'w') as f:\n",
    "        for sec in secs:\n",
    "            f.write('{}\\n'.format(sec))\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "video_file = 'data/videos/helms_deep.mp4'\n",
    "frames_dir = 'data/frames'\n",
    "\n",
    "video_frames_dir = os.path.join(frames_dir, os.path.splitext(os.path.basename(video_file))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_frames_from_video(video_file, video_frames_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нарезаем на сцены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_video_len(video_file):\n",
    "    video_capture = cv2.VideoCapture(video_file)\n",
    "    video_capture.set(cv2.CAP_PROP_POS_AVI_RATIO, 1)\n",
    "    video_len = video_capture.get(cv2.CAP_PROP_POS_MSEC) / 1000\n",
    "    video_capture.release()\n",
    "\n",
    "    return video_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_scenes_timing(video_file, frames_dir, time_step):\n",
    "    scenes_file = os.path.join(frames_dir, 'scenes_{}.txt'.format(time_step))\n",
    "\n",
    "    video_len = get_video_len(video_file)\n",
    "\n",
    "    scene_number = 0\n",
    "\n",
    "    with open(scenes_file, 'w') as f:\n",
    "        start_gris_sec = 0\n",
    "        end_grid_sec = time_step\n",
    "        while end_grid_sec <= video_len:\n",
    "            f.write('scene_{};{};{}\\n'.format(scene_number, float(start_gris_sec), float(end_grid_sec)))\n",
    "            start_gris_sec += time_step\n",
    "            end_grid_sec += time_step\n",
    "            scene_number += 1\n",
    "\n",
    "    return scene_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step = 6\n",
    "save_scenes_timing(video_file, video_frames_dir, time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Находим pos / neg сцены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive scenes: 4, negative scenes: 28\n"
     ]
    }
   ],
   "source": [
    "def overlap(min1, max1, min2, max2):\n",
    "    return max(0, min(max1, max2) - max(min1, min2))\n",
    "            \n",
    "video_gif_secs = [(70, 84), (180, 191)]\n",
    "\n",
    "overlap_thresh = 0.66\n",
    "\n",
    "pos_num = 0\n",
    "neg_num = 0\n",
    "\n",
    "scenes_file = os.path.join(video_frames_dir, 'scenes_{}.txt'.format(time_step))\n",
    "positive_file = os.path.join(video_frames_dir, 'positive_{}.txt'.format(time_step))\n",
    "negative_file = os.path.join(video_frames_dir, 'negative_{}.txt'.format(time_step))\n",
    "frames_timing_file = os.path.join(video_frames_dir, 'frames_timing.txt')\n",
    "\n",
    "frames_timing = []\n",
    "with open(frames_timing_file) as f:\n",
    "    for line in f:\n",
    "        frames_timing.append(float(line.rstrip('\\n')))\n",
    "\n",
    "with open(scenes_file) as f, open(positive_file, 'w') as p_f, open(negative_file, 'w') as n_f:\n",
    "    for line in f:\n",
    "        scene_name, start_sec, end_sec = line.rstrip('\\n').split(';')\n",
    "        start_sec = float(start_sec)\n",
    "        end_sec = float(end_sec)\n",
    "\n",
    "        scene_frames = []\n",
    "        for i, sec in enumerate(frames_timing):\n",
    "            if start_sec <= sec <= end_sec:\n",
    "                scene_frames.append(str(i))\n",
    "\n",
    "        overlaps = []\n",
    "        for sec_pair in video_gif_secs:\n",
    "            len_scene = end_sec - start_sec\n",
    "            len_gif = sec_pair[1] - sec_pair[0]\n",
    "            if len_scene < len_gif:\n",
    "                len_smallest = len_scene\n",
    "            else:\n",
    "                len_smallest = len_gif\n",
    "\n",
    "            if not len_smallest:\n",
    "                continue\n",
    "\n",
    "            ov = overlap(start_sec, end_sec, sec_pair[0], sec_pair[1]) / len_smallest\n",
    "            overlaps.append(ov)\n",
    "\n",
    "        overlaps = np.array(overlaps)\n",
    "        if np.any(overlaps > overlap_thresh):\n",
    "            p_f.write('{};{}\\n'.format(scene_name, ';'.join(scene_frames)))\n",
    "            pos_num += 1\n",
    "        else:\n",
    "            n_f.write('{};{}\\n'.format(scene_name, ';'.join(scene_frames)))\n",
    "            neg_num += 1\n",
    "\n",
    "print('Positive scenes: {}, negative scenes: {}'.format(pos_num, neg_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class C3D(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(C3D, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "\n",
    "        self.conv3a = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv3b = nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "\n",
    "        self.conv4a = nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv4b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "\n",
    "        self.conv5a = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv5b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool5 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1))\n",
    "\n",
    "        self.fc6 = nn.Linear(8192, 4096)\n",
    "        self.fc7 = nn.Linear(4096, 4096)\n",
    "        self.fc8 = nn.Linear(4096, 487)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.conv1(x))\n",
    "        h = self.pool1(h)\n",
    "\n",
    "        h = self.relu(self.conv2(h))\n",
    "        h = self.pool2(h)\n",
    "\n",
    "        h = self.relu(self.conv3a(h))\n",
    "        h = self.relu(self.conv3b(h))\n",
    "        h = self.pool3(h)\n",
    "\n",
    "        h = self.relu(self.conv4a(h))\n",
    "        h = self.relu(self.conv4b(h))\n",
    "        h = self.pool4(h)\n",
    "\n",
    "        h = self.relu(self.conv5a(h))\n",
    "        h = self.relu(self.conv5b(h))\n",
    "        h = self.pool5(h)\n",
    "\n",
    "        h = h.view(-1, 8192)\n",
    "        out = self.fc6(h)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(c3d_path):\n",
    "    embed_net = C3D()\n",
    "    embed_net.load_state_dict(torch.load(c3d_path))\n",
    "\n",
    "#     embed_net = torch.nn.DataParallel(embed_net).cuda()\n",
    "    # embed_net.cuda()\n",
    "\n",
    "    embed_net.eval()\n",
    "\n",
    "    return embed_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x11dbd6e48>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3d_path = 'c3d.pickle'\n",
    "embed_net = get_model(c3d_path)\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисление векторных представлений сцен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from opencv_transforms import opencv_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка сцены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_scene(processing_frames, frames_per_clip=16, overlap_frames=8):\n",
    "    clips_indexes = []\n",
    "    to_copy_clips_ind = defaultdict(int)\n",
    "    if len(processing_frames) < frames_per_clip:\n",
    "        need_frames = frames_per_clip - len(processing_frames)\n",
    "        is_replace = False\n",
    "        if need_frames > len(processing_frames):\n",
    "            is_replace = True\n",
    "\n",
    "        to_copy_clips = np.random.choice(len(processing_frames), need_frames, replace=is_replace)\n",
    "\n",
    "        for i in to_copy_clips:\n",
    "            to_copy_clips_ind[i] += 1\n",
    "\n",
    "        clips_indexes.append(processing_frames)\n",
    "    elif len(processing_frames) > frames_per_clip:\n",
    "        start_ind = 0\n",
    "        end_ind = frames_per_clip\n",
    "\n",
    "        while end_ind <= len(processing_frames):\n",
    "            clips_indexes.append(processing_frames[start_ind:end_ind])\n",
    "            start_ind += overlap_frames\n",
    "            end_ind += overlap_frames\n",
    "\n",
    "        end_ind = len(processing_frames)\n",
    "        start_ind = len(processing_frames) - frames_per_clip\n",
    "        clips_indexes.append(processing_frames[start_ind:end_ind])\n",
    "    else:\n",
    "        clips_indexes.append(processing_frames)\n",
    "\n",
    "    return clips_indexes, to_copy_clips_ind\n",
    "\n",
    "\n",
    "def get_scenes_clips(frames, frames_names, clips_indexes, to_copy_clips_ind, transform,\n",
    "                     frames_per_clip=16, ch=3, width=112, height=112):\n",
    "\n",
    "    clips_tensor = torch.zeros([len(clips_indexes), ch, frames_per_clip, width, height])\n",
    "\n",
    "    for i, ind in enumerate(clips_indexes):\n",
    "        clip = []\n",
    "        for j, frame in enumerate(np.array(sorted(frames_names))[ind]):\n",
    "            name = frames_names[frame]\n",
    "            # img = cv2.cvtColor(frames[name], cv2.COLOR_RGB2BGR)\n",
    "            img = frames[name]\n",
    "            img = transform(img)\n",
    "\n",
    "            clip.append(img)\n",
    "\n",
    "            if j in to_copy_clips_ind:\n",
    "                for _ in range(to_copy_clips_ind[j]):\n",
    "                    clip.append(img)\n",
    "\n",
    "        clip = np.array(clip)\n",
    "        clip = clip.transpose((3, 0, 1, 2))\n",
    "        clip = np.float32(clip)\n",
    "\n",
    "        clips_tensor[i] = torch.from_numpy(clip)\n",
    "\n",
    "    return clips_tensor\n",
    "\n",
    "\n",
    "def get_embeddings(embed_net, clips, frames, frames_names, transform, frames_per_clip=16, overlap_frames=8,\n",
    "                   ch=3, width=112, height=112):\n",
    "    clips_batch = []\n",
    "    for clips_frames in clips:\n",
    "        clips_indexes, to_copy_clips_ind = process_scene(clips_frames, frames_per_clip, overlap_frames)\n",
    "        t_clips = get_scenes_clips(frames, frames_names, clips_indexes, to_copy_clips_ind,\n",
    "                                   transform, frames_per_clip, ch, width, height)\n",
    "\n",
    "        t_clips = t_clips\n",
    "        clips_batch.append(embed_net(t_clips).mean(0).cpu())\n",
    "\n",
    "    return torch.stack(clips_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление и сохранение pos / neg представлений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "frames_per_clip = 16\n",
    "overlap_frames = 8\n",
    "ch = 3\n",
    "width = 112\n",
    "height = 112\n",
    "\n",
    "data_transforms = opencv_transforms.Compose([\n",
    "    opencv_transforms.Resize((112, 200)),\n",
    "    opencv_transforms.CenterCrop(112),\n",
    "])\n",
    "\n",
    "positive_file = os.path.join(video_frames_dir, 'positive_{}.txt'.format(time_step))\n",
    "negative_file = os.path.join(video_frames_dir, 'negative_{}.txt'.format(time_step))\n",
    "\n",
    "pos = []\n",
    "with open(positive_file) as p_f:\n",
    "    for line in p_f:\n",
    "        split_res = line.rstrip('\\n').split(';')\n",
    "        if split_res[0] and split_res[1]:\n",
    "            frames = [int(x) for x in split_res[1:]]\n",
    "            # if len(frames) > args.frames_per_clip:\n",
    "            if len(frames) > frames_per_clip:\n",
    "                pos.append(frames)\n",
    "\n",
    "# if len(pos) == 0:\n",
    "#     continue\n",
    "\n",
    "neg = []\n",
    "with open(negative_file) as n_f:\n",
    "    for line in n_f:\n",
    "        split_res = line.rstrip('\\n').split(';')\n",
    "        if split_res[0] and split_res[1]:\n",
    "            frames = [int(x) for x in split_res[1:]]\n",
    "            # if len(frames) > args.frames_per_clip:\n",
    "            if len(frames) > frames_per_clip:\n",
    "                neg.append(frames)\n",
    "\n",
    "# if len(neg) == 0:\n",
    "#     continue\n",
    "\n",
    "video_frames_file = os.path.join(video_frames_dir,'frames.npz')\n",
    "frames = np.load(video_frames_file)\n",
    "frames_names = {int(x.split('_')[1]): x for x in frames.files}\n",
    "\n",
    "pos_embed_file = os.path.join(video_frames_dir, 'positive_{}_embeddings.pt'.format(time_step))\n",
    "neg_embed_file = os.path.join(video_frames_dir, 'negative_{}_embeddings.pt'.format(time_step))\n",
    "\n",
    "pos_clips_batch = get_embeddings(embed_net, pos, frames, frames_names, data_transforms,\n",
    "                                         frames_per_clip, overlap_frames, ch, width, height)\n",
    "\n",
    "torch.save(pos_clips_batch, pos_embed_file)\n",
    "\n",
    "neg_clips_batch = get_embeddings(embed_net, neg, frames, frames_names, data_transforms,\n",
    "                                 frames_per_clip, overlap_frames, ch, width, height)\n",
    "\n",
    "torch.save(neg_clips_batch, neg_embed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение Rank Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SceneRankNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SceneRankNet, self).__init__()\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=0.8)\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "\n",
    "        self.activ1 = nn.ELU()\n",
    "\n",
    "        self.activ2 = nn.ELU()\n",
    "\n",
    "        self.scoring = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dropout1(x)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.activ1(out)\n",
    "\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        embed = out\n",
    "        out = self.activ2(out)\n",
    "\n",
    "        out = self.scoring(out)\n",
    "\n",
    "        return out, embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank_model(score_net_path):\n",
    "    rank_net = SceneRankNet()\n",
    "\n",
    "    map_location = lambda storage, loc: storage.cpu()\n",
    "    rank_net.load_state_dict(torch.load(score_net_path, map_location=map_location))\n",
    "\n",
    "#     rank_net.cuda()\n",
    "    rank_net.eval()\n",
    "\n",
    "    return rank_net\n",
    "\n",
    "rank_net = get_rank_model('rank_net.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth_l1_loss(positive, negative, delta=1, size_average=False):\n",
    "    u = 1 - positive + negative\n",
    "    value = torch.clamp(u, min=0)\n",
    "\n",
    "    cond = u <= delta\n",
    "    loss = torch.where(cond, 0.5 * value ** 2, delta * value - 0.5 * delta ** 2)\n",
    "    if size_average:\n",
    "        return loss.mean()\n",
    "    return loss.sum()\n",
    "\n",
    "rank_loss = smooth_l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_embed, neg_embed = embeddings\n",
    "\n",
    "pos_batch = torch.stack([embed for embed in pos_embed])\n",
    "neg_batch = torch.stack([embed for embed in neg_embed])\n",
    "\n",
    "pos_scores, pos_score_embed = rank_net(pos_batch)\n",
    "neg_scores, neg_score_embed = rank_net(neg_batch)\n",
    "\n",
    "loss = rank_loss(pos_scores, neg_scores, delta=args.huber_delta, size_average=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка видео и вычисление векторных представлений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_frames_from_video(video_file, new_size=256, fps_factor=3):\n",
    "    video_capture = cv2.VideoCapture(video_file)\n",
    "    video_capture.set(cv2.CAP_PROP_POS_AVI_RATIO, 1)\n",
    "    video_len = video_capture.get(cv2.CAP_PROP_POS_MSEC) / 1000\n",
    "    video_capture.release()\n",
    "\n",
    "    # print('Video len: {}'.format(video_len))\n",
    "\n",
    "    video_capture = cv2.VideoCapture(video_file)\n",
    "\n",
    "    if not video_capture.isOpened():\n",
    "        return 0\n",
    "\n",
    "    frame_num = 0\n",
    "    frames_list = []\n",
    "\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    if fps == 1000 or fps == 0:\n",
    "        print('Bad fps with {}'.format(video_file))\n",
    "        video_capture.release()\n",
    "        return 0\n",
    "\n",
    "    # frame_rate = 1 / fps * fps_factor\n",
    "    frame_rate = 1 / fps_factor\n",
    "    sec = 0\n",
    "\n",
    "    secs = []\n",
    "\n",
    "    while video_capture.isOpened():\n",
    "        video_capture.set(cv2.CAP_PROP_POS_MSEC, sec * 1000)\n",
    "\n",
    "        ret, frame = video_capture.read()\n",
    "\n",
    "        if ret:\n",
    "            frames_list.append(resize_frame(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), new_size))\n",
    "\n",
    "            frame_num += 1\n",
    "            secs.append(sec)\n",
    "\n",
    "            sec = sec + frame_rate\n",
    "            sec = round(sec, 2)\n",
    "\n",
    "            # print(sec)\n",
    "\n",
    "            if sec > video_len:\n",
    "#                 print('Exceeded video length: {} vs {}'.format(sec, video_len))\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            if sec == video_len:\n",
    "                break\n",
    "\n",
    "            if sec < video_len:\n",
    "                if video_len - sec > 1:\n",
    "                    print('Problems with frames splitting: {} vs {}'.format(sec, video_len))\n",
    "                    video_capture.release()\n",
    "                    return 0\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    video_capture.release()\n",
    "\n",
    "    return frames_list, secs, video_len\n",
    "\n",
    "\n",
    "def get_scenes_frames(video_file, last_sec, grid_scene_len=6):\n",
    "    scenes_timing = []\n",
    "    step = grid_scene_len\n",
    "    start_sec = 0\n",
    "    end_sec = step\n",
    "    while end_sec <= last_sec:\n",
    "        scenes_timing.append((float(start_sec), float(end_sec)))\n",
    "        start_sec += step\n",
    "        end_sec += step\n",
    "\n",
    "    return scenes_timing\n",
    "\n",
    "\n",
    "def get_scenes_clips(frames, clips_indexes, to_copy_clips_ind, transform,\n",
    "                     frames_per_clip=16, ch=3, width=112, height=112):\n",
    "\n",
    "    clips_tensor = torch.zeros([len(clips_indexes), ch, frames_per_clip, width, height])\n",
    "\n",
    "    for i, ind in enumerate(clips_indexes):\n",
    "        clip = []\n",
    "        for j, frame in enumerate(np.array(frames)[ind]):\n",
    "            img = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            # img = frame\n",
    "            img = transform(img)\n",
    "\n",
    "            clip.append(img)\n",
    "\n",
    "            if j in to_copy_clips_ind:\n",
    "                for _ in range(to_copy_clips_ind[j]):\n",
    "                    clip.append(img)\n",
    "\n",
    "        clip = np.array(clip)\n",
    "        clip = clip.transpose((3, 0, 1, 2))\n",
    "        clip = np.float32(clip)\n",
    "\n",
    "        clips_tensor[i] = torch.from_numpy(clip)\n",
    "\n",
    "    return clips_tensor\n",
    "\n",
    "\n",
    "def compute_embeddings(embed_net, clips, frames, transform, frames_per_clip=16, overlap_frames=8,\n",
    "                   ch=3, width=112, height=112):\n",
    "    clips_batch = []\n",
    "    scene_process_times = []\n",
    "    embed_net_times = []\n",
    "\n",
    "    for clips_frames in tqdm(clips):\n",
    "        scene_start_time = time.time()\n",
    "        clips_indexes, to_copy_clips_ind = process_scene(clips_frames, frames_per_clip, overlap_frames)\n",
    "        t_clips = get_scenes_clips(frames, clips_indexes, to_copy_clips_ind,\n",
    "                                   transform, frames_per_clip, ch, width, height)\n",
    "        scene_end_time = time.time()\n",
    "        scene_process_times.append(scene_end_time - scene_start_time)\n",
    "\n",
    "#         t_clips = t_clips.cuda()\n",
    "        start_embed_net_time = time.time()\n",
    "        clips_batch.append(embed_net(t_clips).mean(0).cpu())\n",
    "        end_embed_net_time = time.time()\n",
    "        embed_net_times.append(end_embed_net_time - start_embed_net_time)\n",
    "\n",
    "    if len(clips_batch) == 0:\n",
    "        return None, 0, 0\n",
    "\n",
    "    return torch.stack(clips_batch), scene_process_times, embed_net_times\n",
    "\n",
    "def get_embeddings_for_video(video_file, embed_net, data_transforms, grid_scene_len=6):\n",
    "    np.random.seed(3)\n",
    "\n",
    "    frames_per_clip = 16\n",
    "    overlap_frames = 8\n",
    "    ch = 3\n",
    "    width = 112\n",
    "    height = 112\n",
    "\n",
    "    print('\\nVideo file: {}'.format(video_file))\n",
    "    \n",
    "    result = get_frames_from_video(video_file, 256, 3)\n",
    "    if result == 0:\n",
    "        return 0\n",
    "\n",
    "    frames_list, frames_timing, video_len = result\n",
    "\n",
    "    scenes_timing = get_scenes_frames(video_file, video_len, grid_scene_len=grid_scene_len)\n",
    "\n",
    "    significant_scenes_timing = []\n",
    "\n",
    "    scenes_frames = []\n",
    "    for j, (start_sec, end_sec) in enumerate(scenes_timing):\n",
    "        scene_frames = []\n",
    "        for i, sec in enumerate(frames_timing):\n",
    "            if start_sec <= sec <= end_sec:\n",
    "                scene_frames.append(i)\n",
    "\n",
    "        if len(scene_frames) > frames_per_clip:\n",
    "            scenes_frames.append(scene_frames)\n",
    "            significant_scenes_timing.append((start_sec, end_sec))\n",
    "        \n",
    "    scenes_embed_batch, _, __ = compute_embeddings(embed_net, scenes_frames, frames_list, data_transforms,\n",
    "                                                   frames_per_clip, overlap_frames, ch, width, height)\n",
    "\n",
    "    if scenes_embed_batch is None:\n",
    "        return 0\n",
    "\n",
    "    return scenes_embed_batch, significant_scenes_timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x11e46e668>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3d_path = 'c3d.pickle'\n",
    "embed_net = get_model(c3d_path)\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_transforms = opencv_transforms.Compose([\n",
    "        opencv_transforms.Resize((112, 200)),\n",
    "        opencv_transforms.CenterCrop(112),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_folder = 'data/videos' \n",
    "fname = 'bd.mp4'\n",
    "metadata_dir = 'data/frames'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисляем векторные представления видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Video file: data/videos/bd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [01:22<00:00,  4.57s/it]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "grid_scene_len = 6\n",
    "\n",
    "video_name = os.path.splitext(fname)[0]\n",
    "\n",
    "video_file = os.path.join(video_folder, fname)\n",
    "video_data_dir = os.path.join(metadata_dir, video_name)\n",
    "\n",
    "scenes_timing_file = os.path.join(video_data_dir, 'scenes_timing.txt')\n",
    "\n",
    "if not os.path.isdir(video_data_dir):\n",
    "    os.mkdir(video_data_dir)\n",
    "\n",
    "result = get_embeddings_for_video(video_file, embed_net, data_transforms, grid_scene_len=grid_scene_len)\n",
    "\n",
    "clips_batch, scenes_timing = result\n",
    "\n",
    "embed_file = os.path.join(video_data_dir, 'embeddings_{}.pt'.format(grid_scene_len))\n",
    "\n",
    "torch.save(clips_batch, embed_file)\n",
    "\n",
    "with open(scenes_timing_file, 'w') as f:\n",
    "    for start_sec, end_sec in scenes_timing:\n",
    "        f.write('{};{}\\n'.format(start_sec, end_sec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получаем скоры и тайминги хайлайтов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_video_highlights_timing_by_c3d_embeddings(video_metadata_dir, rank_net, grid_scene_len=6):\n",
    "    scenes_timing_file = os.path.join(video_metadata_dir, 'scenes_timing.txt')\n",
    "    embed_file = os.path.join(video_metadata_dir, 'embeddings_{}.pt'.format(grid_scene_len))\n",
    "\n",
    "    scenes_timing = []\n",
    "    with open(scenes_timing_file) as f:\n",
    "        for line in f:\n",
    "            split_res = line.rstrip('\\n').split(';')\n",
    "            scenes_timing.append((float(split_res[0]), float(split_res[1])))\n",
    "\n",
    "    video_len = scenes_timing[-1][1]\n",
    "\n",
    "    scenes_embed_batch = torch.load(embed_file)\n",
    "    scene_scores, embeds = rank_net(scenes_embed_batch)\n",
    "\n",
    "    scene_scores = scene_scores.cpu()\n",
    "\n",
    "    scores = scene_scores.transpose(1, 0)[0]\n",
    "    sort_ind = torch.argsort(scores, descending=True)\n",
    "\n",
    "    highlight_start_sec = []\n",
    "    highlight_end_sec = []\n",
    "    highlight_scenes_scores = []\n",
    "\n",
    "    for i, ind in enumerate(sort_ind):\n",
    "        ind = ind.item()\n",
    "\n",
    "        start_sec, end_sec = scenes_timing[ind]\n",
    "        highlight_start_sec.append(start_sec)\n",
    "        highlight_end_sec.append(end_sec)\n",
    "        highlight_scenes_scores.append(scores[ind].item())\n",
    "\n",
    "    chrono_ind = np.argsort(highlight_start_sec)\n",
    "    highlight_start_sec = np.array(highlight_start_sec)[chrono_ind]\n",
    "    highlight_end_sec = np.array(highlight_end_sec)[chrono_ind]\n",
    "    highlight_scenes_scores = np.array(highlight_scenes_scores)[chrono_ind]\n",
    "\n",
    "    return highlight_start_sec, highlight_end_sec, highlight_scenes_scores, video_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.    6.   12.   18.   24.   30.   36.   42.   48.   54.   60.   66.\n",
      "   72.   78.   84.   90.   96.  102.]\n",
      "[   6.   12.   18.   24.   30.   36.   42.   48.   54.   60.   66.   72.\n",
      "   78.   84.   90.   96.  102.  108.]\n",
      "[ 0.22491862  0.00776201 -0.4874692  -0.4005754  -0.52681738 -0.2362529\n",
      " -0.22521007  0.72785306  0.46509102  0.14945415  0.24952649  0.28643593\n",
      "  0.19895637  0.74111307  0.17376933 -0.0564931   0.18742171 -0.29617655]\n"
     ]
    }
   ],
   "source": [
    "video_metadata_dir = 'data/frames/bd'\n",
    "\n",
    "highlight_start_sec, highlight_end_sec, highlight_scenes_scores, video_len = \\\n",
    "get_video_highlights_timing_by_c3d_embeddings(video_metadata_dir, rank_net, 6)\n",
    "\n",
    "print(highlight_start_sec)\n",
    "print(highlight_end_sec)\n",
    "print(highlight_scenes_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sort_ind = np.argsort(highlight_scenes_scores)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n"
     ]
    }
   ],
   "source": [
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "\n",
    "video_file = 'data/videos/bd.mp4'\n",
    "highlights_dir = 'data/highlights'\n",
    "\n",
    "h_number = 3\n",
    "\n",
    "for i in range(h_number):\n",
    "    highlight_name = 'highlight_{}.mp4'.format(i)\n",
    "    highlight_file = os.path.join(highlights_dir, highlight_name)\n",
    "    ffmpeg_extract_subclip(video_file, highlight_start_sec[sort_ind][i], highlight_end_sec[sort_ind][i],\n",
    "                           targetname=highlight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

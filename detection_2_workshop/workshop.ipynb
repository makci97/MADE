{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Семинар по Detection, Part 2: RetinaNet\n",
    "#### Курс по компьютерному зрению школы MADE."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Useful links:\n",
    "- coco dataset: http://cocodataset.org/#home\n",
    "- based on code: https://github.com/yhenon/pytorch-retinanet\n",
    "- paper Feature Pyramid Networks: https://arxiv.org/pdf/1612.03144.pdf\n",
    "- paper RetinaNet: https://arxiv.org/pdf/1708.02002.pdf\n",
    "- paper EfficientDet: https://arxiv.org/pdf/1911.09070.pdf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RetinaNet\n",
    "FPN + Focal loss\n",
    "\n",
    "## Feature Pyramid Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, sys, cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "# device = torch.cuda.is_available() and 'cuda' or 'cpu'\n",
    "device = 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset - загрузка данных"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from retinanet.dataloader import CocoDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, \\\n",
    "    Normalizer\n",
    "coco_path = '/data2/COCO'\n",
    "# deepdive: what's label ?\n",
    "dataset_train = CocoDataset(coco_path, set_name='train2017',\n",
    "                                    transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]))\n",
    "dataset_val = CocoDataset(coco_path, set_name='val2017',\n",
    "                                  transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "\n",
    "train_imgs, val_imgs = dataset_train.image_ids, dataset_val.image_ids # for hacks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sampler = AspectRatioBasedSampler(dataset_train, batch_size=2, drop_last=False) # deepdive\n",
    "dataloader_train = DataLoader(dataset_train, num_workers=3, collate_fn=collater, batch_sampler=sampler)\n",
    "# ! collater for padding images\n",
    "\n",
    "sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)\n",
    "dataloader_val = DataLoader(dataset_val, num_workers=3, collate_fn=collater, batch_sampler=sampler_val)\n",
    "\n",
    "print(len(dataloader_train), len(dataloader_val)) \n",
    "\n",
    "for i, batch in enumerate(dataloader_train):\n",
    "     img, annot = batch['img'], batch['annot']\n",
    "     print ('img shape', img.shape)\n",
    "     print ('annot shape', annot.shape)\n",
    "     break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('data/sample_batch.json', 'rb') as rf:\n",
    "    batch = torch.load(rf)\n",
    "    img, annot = batch['img'], batch['annot']\n",
    "    \n",
    "print(img.shape) # batch x channel x h x w\n",
    "print(annot.shape) # batch x number of bboxes x (4 points + class label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model = Backbone + FPN + (SubNets + Anchors)\n",
    "\n",
    "## Anchors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Image(filename='images/overall.png')\n",
    "Image(filename='images/acnhor_1.jpg')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from retinanet.anchors import generate_anchors, shift \n",
    "class Anchors(nn.Module):\n",
    "    def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None, device=None):\n",
    "        super(Anchors, self).__init__()\n",
    "\n",
    "        if pyramid_levels is None:\n",
    "            self.pyramid_levels = [3, 4, 5, 6, 7]\n",
    "        if strides is None:\n",
    "            self.strides = [2 ** x for x in self.pyramid_levels]\n",
    "        if sizes is None:\n",
    "            self.sizes = [2 ** (x + 2) for x in self.pyramid_levels]\n",
    "        if ratios is None:\n",
    "            self.ratios = np.array([0.5, 1, 2])\n",
    "        if scales is None:\n",
    "            self.scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "\n",
    "        if device is None:\n",
    "            device = torch.cuda.is_available() and 'cuda' or 'cpu'\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, image):\n",
    "        \n",
    "        image_shape = image.shape[2:] # trim batch and channel\n",
    "        image_shape = np.array(image_shape)\n",
    "        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]\n",
    "\n",
    "        # compute anchors over all pyramid levels\n",
    "        all_anchors = np.zeros((0, 4)).astype(np.float32)\n",
    "\n",
    "        for idx, p in enumerate(self.pyramid_levels):\n",
    "            # generate 9 anchors\n",
    "            anchors         = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)\n",
    "            # grid all over the shape with stride\n",
    "            shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors)\n",
    "            print(self.strides[idx], shifted_anchors.shape)\n",
    "            all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n",
    "\n",
    "        all_anchors = np.expand_dims(all_anchors, axis=0)\n",
    "\n",
    "        return torch.from_numpy(all_anchors.astype(np.float32)).to(self.device)\n",
    "\n",
    "anchors = Anchors(device=device)\n",
    "print('strides', anchors.strides) # '-> areas from 32^2 to 512^2'\n",
    "print('sizes', anchors.sizes) # areas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Image(filename='images/anchor_2.jpg')\n",
    "# Вопрос: почему не сделать разного размера anchor boxes и\n",
    "# предсказывать на одном разрешении ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('ratios', list(anchors.ratios), \"at each pyramid level three aspect ratios {1:2, 1:1, 2:1}\") # \n",
    "print('scales', list(anchors.scales)) # For denser scale coverage than in [20], at each level we add anchors of sizes {2} of the original set of 3 aspect ratio anchors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = np.zeros((0,3,500,800))\n",
    "anc = anchors.forward(image)\n",
    "print('Anchors', anc.shape, anc.device)\n",
    "# print(anc[0,-1,:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Image(filename='images/overall.png')\n",
    "#FPN\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Backbone\n",
    "import torch.nn as nn\n",
    "from retinanet.utils import BasicBlock, Bottleneck # deepdive\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, block, layers):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        if self.training:\n",
    "            img_batch, annotations = inputs\n",
    "        else:\n",
    "            img_batch = inputs\n",
    "        \n",
    "        # To write: everything, returns output of layer2, 3, 4 as features\n",
    "        x = self.conv1(img_batch)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        \n",
    "        return (x2,x3,x4)\n",
    "\n",
    "resnet18_layers = [2, 2, 2, 2]      \n",
    "num_classes = 80 # dataset_train.num_classes()\n",
    "resnet18 = ResNet(num_classes=num_classes, block=BasicBlock, layers=resnet18_layers).to(device)\n",
    "\n",
    "x2,x3,x4 = resnet18((img, annot))\n",
    "print ('x2', x2.size())\n",
    "print ('x3', x3.size())\n",
    "print ('x4', x4.size())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# FPN : make pyramid\n",
    "class PyramidFeatures(nn.Module):\n",
    "    def __init__(self, C3_size, C4_size, C5_size, feature_size=256):\n",
    "        super(PyramidFeatures, self).__init__()\n",
    "\n",
    "        # upsample C5 to get P5 from the FPN paper\n",
    "        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P5_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # add P5 elementwise to C4\n",
    "        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P4_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # add P4 elementwise to C3\n",
    "        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # \"P6 is obtained via a 3x3 stride-2 conv on C5\"\n",
    "        self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # \"P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6\"\n",
    "        self.P7_1 = nn.ReLU()\n",
    "        self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        C3, C4, C5 = inputs\n",
    "\n",
    "        P5_x = self.P5_1(C5)\n",
    "        P5_upsampled_x = self.P5_upsampled(P5_x)\n",
    "        P5_x = self.P5_2(P5_x)\n",
    "\n",
    "        P4_x = self.P4_1(C4)\n",
    "        P4_x = P5_upsampled_x + P4_x\n",
    "        P4_upsampled_x = self.P4_upsampled(P4_x)\n",
    "        P4_x = self.P4_2(P4_x)\n",
    "\n",
    "        # to write: get P3_x\n",
    "        # прокидываем connection от C3\n",
    "        # апсемплим P4 + P3\n",
    "        # P3_x свертываем еще раз\n",
    "        P3_x = self.P3_1(C3)\n",
    "        P3_x = P4_upsampled_x + P3_x\n",
    "        P3_x = self.P3_2(P3_x)        \n",
    "\n",
    "        P6_x = self.P6(C5)\n",
    "\n",
    "        P7_x = self.P7_1(P6_x)\n",
    "        P7_x = self.P7_2(P7_x)\n",
    "\n",
    "        return [P3_x, P4_x, P5_x, P6_x, P7_x]\n",
    "\n",
    "\n",
    "fpn_sizes = [resnet18.layer2[resnet18_layers[1] - 1].conv2.out_channels, resnet18.layer3[resnet18_layers[2] - 1].conv2.out_channels,\n",
    "                         resnet18.layer4[resnet18_layers[3] - 1].conv2.out_channels]\n",
    "fpn = PyramidFeatures(*fpn_sizes).to(device)\n",
    "\n",
    "print('x2,x3,x4 shapes:', x2.shape, x3.shape, x4.shape)\n",
    "features = fpn([x2, x3, x4])\n",
    "\n",
    "for feature in features:\n",
    "    print(feature.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, num_features_in, num_anchors=9, feature_size=256):\n",
    "        super(RegressionModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchors * 4, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.act3(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.act4(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "\n",
    "        # out is B x C x W x H, with C = 4*num_anchors\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        return out.contiguous().view(out.shape[0], -1, 4)\n",
    "\n",
    "regressionModel = RegressionModel(256).to(device)\n",
    "regression = []\n",
    "for feature in features:\n",
    "    regr_at_level = regressionModel(feature)\n",
    "    print('input_shape:', feature.shape, 'output_shape:', regr_at_level.shape)\n",
    "    regression.append(regr_at_level)\n",
    "regression = torch.cat(regression, dim=1) # batch x anchors x 4 [4 for bbox]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Classification subnet\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchors * num_classes, kernel_size=3, padding=1)\n",
    "        self.output_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.act3(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.act4(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "        out = self.output_act(out)\n",
    "\n",
    "        # out is B x C x W x H, with C = n_classes + n_anchors\n",
    "        out1 = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        batch_size, width, height, channels = out1.shape\n",
    "\n",
    "        out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes)\n",
    "\n",
    "        return out2.contiguous().view(x.shape[0], -1, self.num_classes)\n",
    "\n",
    "\n",
    "classificationModel = ClassificationModel(256, num_classes=num_classes).to(device)\n",
    "classification = []\n",
    "print('Classification results')\n",
    "for feature in features:\n",
    "    clf_at_level = classificationModel(feature)\n",
    "    print('input_shape:', feature.shape, 'output_shape:', clf_at_level.shape)\n",
    "    classification.append(clf_at_level)\n",
    "classification = torch.cat(classification, dim=1) # batch x anchors x classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Focal loss\n",
    "Вопрос: с какой целью его используют?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# p_t = y == 1 and p or y == -1 and 1 - p\n",
    "# alpha_t = y == 1 and alpha or 1 - alpha \n",
    "Image(filename='images/focal loss.jpg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# all together\n",
    "anchors_res = anchors(img)\n",
    "print (regression.shape, anchors_res.shape, annot.shape)\n",
    "from retinanet.losses import FocalLoss # deepdive\n",
    "focalLoss = FocalLoss(device=device)\n",
    "\n",
    "# here for regression and classification\n",
    "result = focalLoss(classification, regression, anchors_res, annot)\n",
    "print ('loss', result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Image(filename='images/smooth-l1.png')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Image(filename='images/predictions.jpg')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inference\n",
    "from torchvision.ops import nms\n",
    "from retinanet.utils import BBoxTransform, ClipBoxes\n",
    "regressBoxes = BBoxTransform(device=device)\n",
    "clipBoxes = ClipBoxes()\n",
    "\n",
    "transformed_anchors = regressBoxes(anchors_res, regression) # convert\n",
    "transformed_anchors = clipBoxes(transformed_anchors, img) # clip\n",
    "\n",
    "scores = torch.max(classification, dim=2, keepdim=True)[0]\n",
    "print(scores.shape)\n",
    "\n",
    "scores_over_thresh = (scores > 0.05)[0, :, 0] \n",
    "\n",
    "if scores_over_thresh.sum() == 0:\n",
    "    # no boxes to NMS, just return\n",
    "    print('result',[torch.zeros(0), torch.zeros(0), torch.zeros(0, 4)])\n",
    "else:\n",
    "    # NMS iteratively removes lower scoring boxes which have an\n",
    "    #     IoU greater than iou_threshold with another (higher scoring) box\n",
    "    classification = classification[:, scores_over_thresh, :]\n",
    "    transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n",
    "    scores = scores[:, scores_over_thresh, :]\n",
    "    \n",
    "    anchors_nms_idx = nms(transformed_anchors[0,:,:], scores[0,:,0], iou_threshold=0.5)\n",
    "    \n",
    "    nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(dim=1)\n",
    "    print('results', nms_scores.shape, nms_class.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from retinanet import model\n",
    "retina_model = model.resnet18(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "# retina_model.load_state_dict(torch.load('data/coco_resnet_50_map_0_335_state_dict.pt'))\n",
    "\n",
    "device = torch.cuda.is_available() and 'cuda' or 'cpu'\n",
    "retina_model = retina_model.to(device=device)\n",
    "# \n",
    "# for param in retina_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# \n",
    "\n",
    "params = [p for p in retina_model.parameters() if p.requires_grad]\n",
    "retina_model = torch.nn.DataParallel(retina_model).to(device=device)\n",
    "     \n",
    "print('Done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare\n",
    "import collections\n",
    "import torch.optim as optim\n",
    "dataset_val.image_ids = val_imgs[:100]  # TEST\n",
    "# dataset_train.image_ids = train_imgs\n",
    "retina_model.training = True\n",
    "\n",
    "optimizer = optim.Adam(params, lr=1e-5)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "\n",
    "loss_hist = collections.deque(maxlen=500)\n",
    "\n",
    "print('Num training images: {}'.format(len(dataset_train)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from retinanet import coco_eval\n",
    "\n",
    "def routine(retina_model, scheduler):\n",
    "    retina_model.train()\n",
    "    retina_model.module.freeze_bn()\n",
    "\n",
    "    epoch_loss = []\n",
    "\n",
    "    for iter_num, data in enumerate(dataloader_train):\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "            classification_loss, regression_loss = retina_model([data['img'].to(device).float(), data['annot']])\n",
    "                \n",
    "            classification_loss = classification_loss.mean()\n",
    "            regression_loss = regression_loss.mean()\n",
    "\n",
    "            loss = classification_loss + regression_loss\n",
    "\n",
    "            if bool(loss == 0):\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(retina_model.parameters(), 0.1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_hist.append(float(loss))\n",
    "\n",
    "            epoch_loss.append(float(loss))\n",
    "            if iter_num % 10 == 0:\n",
    "                print(\n",
    "                    'Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f}'.format(\n",
    "                        epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist)))\n",
    "\n",
    "            del classification_loss\n",
    "            del regression_loss\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "            \n",
    "        # if iter_num > 1000:\n",
    "        #     break\n",
    "\n",
    "\n",
    "    print('Evaluating dataset')\n",
    "    coco_eval.evaluate_coco(dataset_val, retina_model)\n",
    "    \n",
    "    scheduler.step(np.mean(epoch_loss)) # should be val loss\n",
    "    torch.save(retina_model.module, 'dev_retinanet_{}.pt'.format(epoch_num))\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# routine\n",
    "epochs = 10\n",
    "for epoch_num in range(epochs):\n",
    "    routine(retina_model, scheduler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Efficient det\n",
    "\n",
    "Main difference from RetinaNet - EfficientNet as a Backbone + BiFPN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Image(filename='images/bifpn.png')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}